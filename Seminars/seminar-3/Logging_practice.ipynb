{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment tracking (logging) in PyTorch.\n"
      ],
      "metadata": {
        "id": "3jNJ_e1qcqm1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will get acquainted with several logging options ([Wandb](https://wandb.ai), [TensorBoard](https://www.tensorflow.org/tensorboard))"
      ],
      "metadata": {
        "id": "1nTOYLUQcOME"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wandb"
      ],
      "metadata": {
        "id": "i6qFOQiW-9xX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb --quiet\n",
        "!wandb login"
      ],
      "metadata": {
        "id": "L1EA6HzbCKA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Toy example"
      ],
      "metadata": {
        "id": "3o-xe7mJ_EWe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import wandb\n",
        "\n",
        "config = dict(\n",
        "    project_name = \"Test wandb\",\n",
        "    learning_rate = 0.02,\n",
        "    architecture = \"CNN\",\n",
        "    dataset = \"CIFAR-100\",\n",
        "    epochs = 10,\n",
        "    total_runs = 5\n",
        ")\n",
        "\n",
        "for run in range(config[\"total_runs\"]):\n",
        "  # üêù 1Ô∏è‚É£ Start a new run to track this script\n",
        "  wandb.init(\n",
        "      # Set the project where this run will be logged\n",
        "      project=config[\"project_name\"], \n",
        "      # We pass a run name (otherwise it‚Äôll be randomly assigned)\n",
        "      name=f\"experiment_{run}\", \n",
        "      # Track hyperparameters and run metadata\n",
        "      config=config\n",
        "      )\n",
        "  \n",
        "  # This simple block simulates a training loop logging metrics\n",
        "  offset = random.random() / 5\n",
        "  for epoch in range(2, config[\"epochs\"]):\n",
        "      acc = 1 - 2 ** -epoch - random.random() / epoch - offset\n",
        "      loss = 2 ** -epoch + random.random() / epoch + offset\n",
        "      \n",
        "      # üêù 2Ô∏è‚É£ Log metrics from your script to W&B\n",
        "      wandb.log({\n",
        "          \"acc\": acc, \n",
        "          \"loss\": loss\n",
        "          })\n",
        "      \n",
        "  # Mark the run as finished\n",
        "  wandb.finish()"
      ],
      "metadata": {
        "id": "Ci5eWhl5B6X4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Neural network training"
      ],
      "metadata": {
        "id": "aZEhOzYg_GSb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "import wandb\n",
        "import math\n",
        "import random\n",
        "import torch, torchvision\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as T\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def get_dataloader(is_train, batch_size, slice=5):\n",
        "    \"Get a training dataloader\"\n",
        "    full_dataset = torchvision.datasets.FashionMNIST(\n",
        "        root=\".\", \n",
        "        train=is_train, \n",
        "        transform=T.ToTensor(), \n",
        "        download=True\n",
        "        )\n",
        "    sub_dataset = torch.utils.data.Subset(\n",
        "        full_dataset, \n",
        "        indices=range(0, len(full_dataset), slice)\n",
        "        )\n",
        "    loader = torch.utils.data.DataLoader(dataset=sub_dataset, \n",
        "                                         batch_size=batch_size, \n",
        "                                         shuffle=True if is_train else False, \n",
        "                                         pin_memory=True, num_workers=2)\n",
        "    return loader\n",
        "\n",
        "def output_label(label):\n",
        "    output_mapping = {\n",
        "                 0: \"T-shirt/Top\",\n",
        "                 1: \"Trouser\",\n",
        "                 2: \"Pullover\",\n",
        "                 3: \"Dress\",\n",
        "                 4: \"Coat\", \n",
        "                 5: \"Sandal\", \n",
        "                 6: \"Shirt\",\n",
        "                 7: \"Sneaker\",\n",
        "                 8: \"Bag\",\n",
        "                 9: \"Ankle Boot\"\n",
        "                 }\n",
        "    input = (label.item() if type(label) == torch.Tensor else label)\n",
        "    return output_mapping[input]\n",
        "\n",
        "def get_model(dropout):\n",
        "    \"A simple model\"\n",
        "    model = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(28*28, 256),\n",
        "        nn.BatchNorm1d(256),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(dropout),\n",
        "        nn.Linear(256,10)\n",
        "        ).to(device)\n",
        "    return model\n",
        "\n",
        "def validate_model(model, valid_dl, loss_func, log_images=False, batch_idx=0):\n",
        "    \"Compute performance of the model on the validation data, log a wandb.Table\"\n",
        "    model.eval()\n",
        "    val_loss = 0.\n",
        "    with torch.inference_mode():\n",
        "        correct = 0\n",
        "        for i, (images, labels) in enumerate(valid_dl):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass ‚û°\n",
        "            outputs = model(images)\n",
        "            val_loss += loss_func(outputs, labels)*labels.size(0)\n",
        "\n",
        "            # Compute accuracy and accumulate\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            # Log one batch of images to the dashboard, always same batch_idx.\n",
        "            if i==batch_idx and log_images:\n",
        "                log_image_table(images, predicted, labels, \n",
        "                                outputs.softmax(dim=1))\n",
        "    return val_loss / len(valid_dl.dataset), correct / len(valid_dl.dataset)\n",
        "\n",
        "def log_image_table(images, predicted, labels, probs):\n",
        "    \"Log a wandb.Table with (img, pred, target, scores)\"\n",
        "    # üêù Create a wandb Table to log images, labels and predictions to\n",
        "    table = wandb.Table(columns=[\"image\", \"pred\", \"target\"]+\n",
        "                        [f\"score_{i}\" for i in range(10)])\n",
        "    for img, pred, targ, prob in zip(images.to(\"cpu\"), \n",
        "                                     predicted.to(\"cpu\"), \n",
        "                                     labels.to(\"cpu\"), \n",
        "                                     probs.to(\"cpu\")):\n",
        "        table.add_data(wandb.Image(img[0].numpy()*255), \n",
        "                                output_label(pred), \n",
        "                                output_label(targ), \n",
        "                                *prob.numpy())\n",
        "    wandb.log({\"predictions_table\":table}, commit=False)"
      ],
      "metadata": {
        "id": "-jbBt_r5p6dl"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Launch 5 experiments, trying different dropout rates\n",
        "for _ in range(5):\n",
        "    # üêù initialise a wandb run\n",
        "    wandb.init(\n",
        "        project=\"pytorch-intro\",\n",
        "        config={\n",
        "            \"epochs\": 10,\n",
        "            \"batch_size\": 128,\n",
        "            \"lr\": 1e-3,\n",
        "            \"dropout\": random.uniform(0.01, 0.80)\n",
        "            })\n",
        "    \n",
        "    # Copy your config \n",
        "    config = wandb.config\n",
        "\n",
        "    # Get the data\n",
        "    train_dl = get_dataloader(is_train=True, batch_size=config.batch_size)\n",
        "    valid_dl = get_dataloader(is_train=False, batch_size=2*config.batch_size)\n",
        "    n_steps_per_epoch = math.ceil(len(train_dl.dataset) / config.batch_size)\n",
        "    \n",
        "    # A simple MLP model\n",
        "    model = get_model(config.dropout)\n",
        "\n",
        "    # Make the loss and optimizer\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n",
        "\n",
        "   # Training\n",
        "    example_ct = 0\n",
        "    step_ct = 0\n",
        "    for epoch in range(config.epochs):\n",
        "        model.train()\n",
        "        for step, (images, labels) in enumerate(train_dl):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            train_loss = loss_func(outputs, labels)\n",
        "            optimizer.zero_grad()\n",
        "            train_loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            example_ct += len(images)\n",
        "            metrics = {\"train/train_loss\": train_loss, \n",
        "                       \"train/epoch\": (step +\n",
        "                        1 + (n_steps_per_epoch * epoch)) / n_steps_per_epoch, \n",
        "                       \"train/example_ct\": example_ct}\n",
        "            \n",
        "            if step + 1 < n_steps_per_epoch:\n",
        "                # üêù Log train metrics to wandb \n",
        "                wandb.log(metrics)\n",
        "                \n",
        "            step_ct += 1\n",
        "\n",
        "        val_loss, accuracy = validate_model(\n",
        "                                        model, \n",
        "                                        valid_dl, \n",
        "                                        loss_func, \n",
        "                                        log_images=(epoch==(config.epochs-1))\n",
        "                                        )\n",
        "\n",
        "        # üêù Log train and validation metrics to wandb\n",
        "        val_metrics = {\"val/val_loss\": val_loss, \n",
        "                       \"val/val_accuracy\": accuracy}\n",
        "        wandb.log({**metrics, **val_metrics})\n",
        "        \n",
        "        print(f\"ü§ñ Train Loss: {train_loss:.3f}, Valid Loss: {val_loss:3f}, \"\n",
        "            f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "    # If you had a test set, this is how you could log it as a Summary metric\n",
        "    wandb.summary['test_accuracy'] = 0.8\n",
        "\n",
        "    # üêù Close your wandb run \n",
        "    wandb.finish()"
      ],
      "metadata": {
        "id": "waW89gBnp876"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "0. Here we can plot the graph for the generalization error, play with the legend.\n",
        "\n",
        "1. Make two runs with the same dropout rate. Make conclusion."
      ],
      "metadata": {
        "id": "1Foj0uoJ2IAO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def fix_seed(seed=42):\n",
        "    # torch.use_deterministic_algorithms(True)\n",
        "    torch.manual_seed(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)"
      ],
      "metadata": {
        "id": "8xFOXBwy4pn7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Compare SGD and Adam for this task within 3 runs"
      ],
      "metadata": {
        "id": "eVZYl2hx5CU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "methods = [\"SGD\", \"Adam\"]\n",
        "N_runs = 3\n",
        "for method in methods:\n",
        "    # Launch N_runs experiments, trying different dropout rates\n",
        "    for _ in range(N_runs):\n",
        "        fix_seed(_)\n",
        "        # üêù initialise a wandb run\n",
        "        wandb.init(\n",
        "            # reinit=True,\n",
        "            project=\"pytorch-intro\",\n",
        "            config={\n",
        "                \"epochs\": 10,\n",
        "                \"batch_size\": 128,\n",
        "                \"dropout\": 0.1,\n",
        "                \"method\": method\n",
        "                })\n",
        "        \n",
        "        # Copy your config \n",
        "        config = wandb.config\n",
        "\n",
        "        # Get the data\n",
        "        train_dl = get_dataloader(is_train=True, batch_size=config.batch_size)\n",
        "        valid_dl = get_dataloader(is_train=False, batch_size=2*config.batch_size)\n",
        "        n_steps_per_epoch = math.ceil(len(train_dl.dataset) / config.batch_size)\n",
        "        \n",
        "        # A simple MLP model\n",
        "        model = get_model(config.dropout)\n",
        "\n",
        "        # Make the loss and optimizer\n",
        "        loss_func = nn.CrossEntropyLoss()\n",
        "        if method == \"Adam\":\n",
        "            config.lr = 1e-3\n",
        "            optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n",
        "        elif method == \"SGD\":\n",
        "            config.lr = 5e-2\n",
        "            optimizer = torch.optim.SGD(model.parameters(), lr=config.lr)\n",
        "\n",
        "    # Training\n",
        "        example_ct = 0\n",
        "        step_ct = 0\n",
        "        for epoch in range(config.epochs):\n",
        "            model.train()\n",
        "            for step, (images, labels) in enumerate(train_dl):\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "                outputs = model(images)\n",
        "                train_loss = loss_func(outputs, labels)\n",
        "                optimizer.zero_grad()\n",
        "                train_loss.backward()\n",
        "                optimizer.step()\n",
        "                \n",
        "                example_ct += len(images)\n",
        "                metrics = {\"train/train_loss\": train_loss, \n",
        "                        \"train/epoch\": (step +\n",
        "                        1 + (n_steps_per_epoch * epoch)) / n_steps_per_epoch, \n",
        "                        \"train/example_ct\": example_ct}\n",
        "                \n",
        "                if step + 1 < n_steps_per_epoch:\n",
        "                    # üêù Log train metrics to wandb \n",
        "                    wandb.log(metrics)\n",
        "                    \n",
        "                step_ct += 1\n",
        "\n",
        "            val_loss, accuracy = validate_model(\n",
        "                                            model, \n",
        "                                            valid_dl, \n",
        "                                            loss_func, \n",
        "                                            log_images=(epoch==(config.epochs-1))\n",
        "                                            )\n",
        "\n",
        "            # üêù Log train and validation metrics to wandb\n",
        "            val_metrics = {\"val/val_loss\": val_loss, \n",
        "                        \"val/val_accuracy\": accuracy}\n",
        "            wandb.log({**metrics, **val_metrics})\n",
        "            \n",
        "            print(f\"Train Loss: {train_loss:.3f}, Valid Loss: {val_loss:3f}, \"\n",
        "            f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "        # If you had a test set, this is how you could log it as a Summary metric\n",
        "        wandb.summary['test_accuracy'] = 0.8\n",
        "\n",
        "        # üêù Close your wandb run \n",
        "        wandb.finish()"
      ],
      "metadata": {
        "id": "sCH8T4cE5Uau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Add momentum to the SGD and try 3 different values of momentum with 3 runs per each setting and compare it with previous results"
      ],
      "metadata": {
        "id": "CjlnIvmeNghT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accessing data logged in wandb"
      ],
      "metadata": {
        "id": "BS5ApNojfZgm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "import pickle \n",
        "\n",
        "def save_obj(obj, name ):\n",
        "    with open(name + '.pkl', 'wb') as f:\n",
        "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "def load_obj(name ):\n",
        "    with open(name + '.pkl', 'rb') as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "import wandb\n",
        "wandb.login()\n",
        "\n",
        "entity         = \"skoltech_optimization\" # team name\n",
        "project         = 'Test wandb' # mtl_mcifar10, mtl_mcifar10_adaptive\n",
        "datafile        = 'wandb_data'\n",
        "logged_metrics  = [\"acc\",\n",
        "                  \"loss\"]\n",
        "\n",
        "api     = wandb.Api()\n",
        "runs    = api.runs(f\"{entity}/{project}\")\n",
        "\n",
        "# try: \n",
        "#   global_history_information = load_obj(datafile)\n",
        "# except TypeError: \n",
        "#   global_history_information = {}\n",
        "# else: \n",
        "#   print(f'Updating current object')\n",
        "\n",
        "global_history_information = {}\n",
        "\n",
        "for run in runs:\n",
        "    # Avoid obsolete API requests\n",
        "    if run.id in global_history_information.keys():\n",
        "        continue\n",
        "        \n",
        "    global_history_information[run.id] = {}\n",
        "    global_history_information[run.id]['name']      = run.name\n",
        "    for metric in logged_metrics:\n",
        "            global_history_information[run.id][metric] = []\n",
        "    print(f'ü§ñ Requesting metrics for {project}. {run.notes}')\n",
        "    for dict_item in run.scan_history():\n",
        "        for metric in logged_metrics:\n",
        "            if metric in dict_item.keys():\n",
        "                global_history_information[run.id][metric].append(dict_item[metric])     \n",
        "        \n",
        "   \n",
        "save_obj(global_history_information, datafile)"
      ],
      "metadata": {
        "id": "4sYSq5rgfd4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for run_id in global_history_information:\n",
        "    print(global_history_information[run_id])"
      ],
      "metadata": {
        "id": "yBnmfhxbhAVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensorboard"
      ],
      "metadata": {
        "id": "EkHcCG8G-_oH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic usage"
      ],
      "metadata": {
        "id": "RWcOMphL_CAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipiRgR0-Ht4Y",
        "outputId": "d66a837c-9bd1-4073-f125-48e2de3c7c6e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "log_dir = \"logs\"\n",
        "writer = SummaryWriter(log_dir)"
      ],
      "metadata": {
        "id": "cWd67BzDJY0F"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.arange(-5, 5, 0.1).view(-1, 1)\n",
        "y = -5 * x + 0.1 * torch.randn(x.size())\n",
        "\n",
        "model = torch.nn.Linear(1, 1)\n",
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 0.1)\n",
        "\n",
        "def train_model(iter):\n",
        "    for epoch in range(iter):\n",
        "        y1 = model(x)\n",
        "        loss = criterion(y1, y)\n",
        "        writer.add_scalar(\"Loss/train\", loss, epoch)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "train_model(10)\n",
        "writer.flush()"
      ],
      "metadata": {
        "id": "VGH1GfhXJblg"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "writer.close()"
      ],
      "metadata": {
        "id": "NXyzTkTiJd3Q"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir=logs"
      ],
      "metadata": {
        "id": "Qtj15I23JgPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Useful links and sources**\n",
        "* [Wandb tutorial](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/intro/Intro_to_Weights_%26_Biases.ipynb#scrollTo=7E95A_hhlSNx)\n",
        "* [Wandb legend expression docs](https://docs.wandb.ai/guides/app/features/panels/line-plot/reference#legend)\n",
        "* [Getting Started with Tensorboard](https://www.tensorflow.org/tensorboard/get_started)\n",
        "* [PyTorch and TensorBoard](https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html)\n",
        "* [Neptune](https://neptune.ai) - another tool with similar usecases"
      ],
      "metadata": {
        "id": "q5UQYOkB9uDB"
      }
    }
  ]
}