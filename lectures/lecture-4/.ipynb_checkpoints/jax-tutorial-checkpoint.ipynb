{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d6a4f14",
   "metadata": {},
   "source": [
    "# JAX Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d8886f",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "\n",
    "1. [Introduction][1].\n",
    "2. [General array and algebra routines (NumPy-like)][2].\n",
    "    1. Array creation routines.\n",
    "    2. Array indexing.\n",
    "    3. Array mutation.\n",
    "3. Random Number generators.\n",
    "4. Device placement management.\n",
    "5. Composable Transformations\n",
    "    1. jit\n",
    "    2. vmap\n",
    "    3. pmap\n",
    "    4. checkify\n",
    "6. Control flow and static arguments.\n",
    "7. Tree utils.\n",
    "8. Custom types in JAX.\n",
    "9. Custom primitives.\n",
    "10. Common gotchas.\n",
    "11. Debugging (id_print).\n",
    "\n",
    "[1]: #1-Introduction\n",
    "[2]: #2-Arrays-Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27599a94",
   "metadata": {},
   "source": [
    "## 1 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830d1084",
   "metadata": {},
   "source": [
    "### What tools did ancestors use?\n",
    "\n",
    "- Low-level programming languages like Fortran or C/C++. It's bad: a lot of bugs, slow development, requires knowledge about hardware or OS internals.\n",
    "- High-level programming languages like Matlab. It's a litte bit better: faster development, interactive development, slow execution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249462aa",
   "metadata": {},
   "source": [
    "### What is NumPy?\n",
    "\n",
    "- Performant numerical algebra routines in Python.\n",
    "- Lowering to common BLAS/LAPACK (e.g. Intel oneAPI aka MKL).\n",
    "- Inspired by Matlab and Fortran.\n",
    "- Implimented as pure Python library with a set of Python extensions written in plain C.\n",
    "\n",
    "**NB** NumPy is about 20 years old! It has a great impact on Python (see [Buffer protocol][4] or operator `@`) and an entier area of scientific computations (see [Matplotlib][2] or [SciPy][3]).\n",
    "\n",
    "[1]: https://github.com/numpy/numpy\n",
    "[2]: https://github.com/matplotlib/matplotlib\n",
    "[3]: https://github.com/scipy/scipy\n",
    "[4]: https://docs.python.org/3/c-api/buffer.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f191ffb",
   "metadata": {},
   "source": [
    "### What is JAX?\n",
    "\n",
    "- \"Next-generation\" NumPy.\n",
    "- Advanced support for native hardware: optimization and compilation to the actual hardware (JIT-compilation).\n",
    "- Advanced support for math accelerator (GPU or TPU).\n",
    "- Autograd out of the box (e.g. `jax.grad`, `jax.jvp` or `jax.vjp`).\n",
    "- Distributed execution out of the box (`jax.distributed` and `jax.pmap`).\n",
    "- Composable transformation (e.g. `jax.vmap(jax.jit(jax.vmap(jax.grad(...))))` can be easily implemented in extensible way)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cb1d1c",
   "metadata": {},
   "source": [
    "### What alternatives exist?\n",
    "\n",
    "- Use high-level Python libraries. TensorFlow, PyTorch, Theano, Numba, CuPy are well-known examples.\n",
    "- We still can write Python extensions in \"low-level\" programming languages like C/C++, Rust, Fortran.\n",
    "- Do everying in other \"high-level\" language like Julia, R, Matlab, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebba6278",
   "metadata": {},
   "source": [
    "### What is the future?\n",
    "\n",
    "- One can see convergance of different tools to a common set of ideas or base technologies. For example, Numba, Julia, TensorFlow, JAX, flang (LLVM's Fortran) are based on LLVM compiler infrastructure.\n",
    "- Common execution runtimes exist for diffrent frameworks e.g. IREE or ONNX.\n",
    "- Different libraries/lagnguages influence each other (e.g. JAX causes arrival of [functorch][1]).\n",
    "- Other proof-of-concept experimental languages like [DEX][2].\n",
    "\n",
    "[1]: https://github.com/pytorch/functorch\n",
    "[2]: https://github.com/google-research/dex-lang"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2508da40",
   "metadata": {},
   "source": [
    "## 2 Arrays Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029d2d75",
   "metadata": {},
   "source": [
    "In general one can install NumPy and JAX as follows (for advanced installations see [documentation][1]).\n",
    "\n",
    "[1]: https://github.com/google/jax#installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffd5603",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ed50dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213add8b",
   "metadata": {},
   "source": [
    "From here onwards we use the imports below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d1df6a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "853f8b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbaf1df6",
   "metadata": {},
   "source": [
    "### 2.1 Array Creation Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e352385",
   "metadata": {},
   "source": [
    "The simplest way to create array is a "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2808d0d1",
   "metadata": {},
   "source": [
    "![alt](img/np_array.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e17f6909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.array([1, 2, 3])\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "830dfa17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "de8960f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e7d73a80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0.])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bc8895fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1.],\n",
       "       [1., 1., 1.]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones((2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541f6af5",
   "metadata": {},
   "source": [
    "We can do the same in JAX! We need only to replace `np` with `jnp`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b02e9f",
   "metadata": {},
   "source": [
    "### 2.2 Array Algebra Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a4af61",
   "metadata": {},
   "source": [
    "![](img/np_sub_mult_divide.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7efc1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([1, 2])\n",
    "ones = np.ones_like(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05b676f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data - ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8fef9c97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data * ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e2dbe0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 2.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data / ones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27106195",
   "metadata": {},
   "source": [
    "Again, we can do the same in JAX as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "17ea0e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = jnp.array([1, 2])\n",
    "ones = jnp.ones_like(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "de3d41ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'jax.numpy' has no attribute 'recarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecarray\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'jax.numpy' has no attribute 'recarray'"
     ]
    }
   ],
   "source": [
    "jnp.recarray  # There is not analogue to numpy.recarray in JAX."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b732db91",
   "metadata": {},
   "source": [
    "gauss ellimination\n",
    "\n",
    "laplace operator with diriclet conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9783738e",
   "metadata": {},
   "source": [
    "TODO: Common difference is data types!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d982dc6",
   "metadata": {},
   "source": [
    "### 2.3. Array Indexing Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e0884c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6517078e",
   "metadata": {},
   "source": [
    "## 3 (Pseudo) Random Number Generators aka (P)RNG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810ee586",
   "metadata": {},
   "source": [
    "Let's draw samples from normal ditribution with NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "278f5849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.04681043,  0.0079646 , -0.03151406],\n",
       "       [ 0.77888352,  0.08112633, -0.80160754]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.normal(size=(2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ca92ce",
   "metadata": {},
   "source": [
    "It does not works in JAX. The issue is that a RNG has a state but in the same we expect that any computation is\n",
    "\n",
    "- reproducible,\n",
    "- parallelizable,\n",
    "- vectorisable.\n",
    "\n",
    "Consider linear congruential generator (LCG) which is \n",
    "\n",
    "$$\n",
    "    x_{n + 1} = a x_n + b \\quad (\\mathrm{mod} M).\n",
    "$$\n",
    "Its state is two numbers $a$ and $b$. Note, LCG is a bad random number generator. If you needs speed condifer `xoroshiro`-like RNGS.\n",
    "\n",
    "**Problem.** How to generate random $K$ numbers in parallel in two or on device? Skip $K$ numbers and then take next $K$? Replicate somehow RNGs and seed them with different initial seeds? How does this seeding influece quality of pseudorandom?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9addde92",
   "metadata": {},
   "source": [
    "Use explicit random state in NumPy for sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7dceb8cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomState(MT19937) at 0x7F8FF4488B40"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng = np.random.RandomState(42)\n",
    "rng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b463905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.49671415, -0.1382643 ,  0.64768854],\n",
       "       [ 1.52302986, -0.23415337, -0.23413696]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng.normal(size=(2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67b9bcbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.57921282,  0.76743473, -0.46947439],\n",
       "       [ 0.54256004, -0.46341769, -0.46572975]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng.normal(size=(2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b922a728",
   "metadata": {},
   "source": [
    "Almost the same true for JAX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be32f817",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e45d25de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([ 0, 42], dtype=uint32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(42)\n",
    "key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2df18426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[ 0.6122652 ,  1.1225883 ,  1.1373317 ],\n",
       "             [-0.8127325 , -0.890405  ,  0.12623145]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.random.normal(key, (2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fdd2c887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[ 0.6122652 ,  1.1225883 ,  1.1373317 ],\n",
       "             [-0.8127325 , -0.890405  ,  0.12623145]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.random.normal(key, (2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693d2c72",
   "metadata": {},
   "source": [
    "However, we can split state of the RNG as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dee81ddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[-0.55338794,  0.944283  ,  0.14538097],\n",
       "             [-0.5769758 ,  1.1251862 ,  1.0788847 ]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key, subkey = jax.random.split(key)\n",
    "jax.random.normal(subkey, (2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "31629737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample #1: 2.4850724\n",
      "sample #2: -1.0498956\n",
      "sample #3: 0.17443566\n",
      "sample #4: 0.7091227\n",
      "sample #5: 1.6366044\n",
      "sample #6: -0.43166864\n",
      "sample #7: -1.2532382\n",
      "sample #8: 1.7160755\n"
     ]
    }
   ],
   "source": [
    "key, *subkeys = jax.random.split(key, 9)\n",
    "for i, subkey in enumerate(subkeys, 1):\n",
    "    print(f'sample #{i}:', jax.random.normal(subkey))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff66d00",
   "metadata": {},
   "source": [
    "### 4 Device Placement Management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e26668",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2e0d6de",
   "metadata": {},
   "source": [
    "### 5 Composable Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4c7081",
   "metadata": {},
   "source": [
    "The most powerfull and non-trivial feature of JAX is easy in use and implemenation a composition of transformations. Say, we write transformatoins `jax.jit` and `jax.grad`. They can be implimented in a quite common way (e.g. JIT-compilation with `jax.jit` can be implemented as direct tree traversing). But implementation both transformations become more complicated. Moreover, difficulty exponentially increases as a number of different transformations increases. JAX authors solve the issue with so called tagless final encoding or just tagless final (see [Kiselyov paper][1]).\n",
    "\n",
    "In this section we will try some transformations on practice which JAX suggests.\n",
    "\n",
    "[1]: https://okmij.org/ftp/tagless-final/#tagless-final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aaee924",
   "metadata": {},
   "source": [
    "### 1 JIT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f29f55",
   "metadata": {},
   "source": [
    "Probably, this is the most famous transformation. It evaluate JAX code in an abstract way and produces native code which can be run directly on CPU, GPU, or TPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "230d6f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selu(x, alpha=1.67, lambda_=1.05):\n",
    "    return lambda_ * jnp.where(x > 0, x, alpha * jnp.exp(x) - alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "74b49827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56.8 µs ± 6.34 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "inp = np.arange(100.)\n",
    "%timeit selu(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "476446fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46.5 µs ± 3.72 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "inp = jnp.arange(100.)\n",
    "%timeit selu(inp).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "3492d529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.87 µs ± 888 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "selu_jit = jax.jit(selu)\n",
    "inp = jnp.arange(100.)\n",
    "%timeit selu_jit(inp).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08de875f",
   "metadata": {},
   "source": [
    "**NOTE** Do not forget to wait until computations end."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88344137",
   "metadata": {},
   "source": [
    "**NOTE** It may be hard to find speed up for simple function but on large ones like large neural networks (e.g. BERT or GPT) speed up can be significant even in comparison to other deep learning framework. The diffirence is up to 20%!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e73bdd",
   "metadata": {},
   "source": [
    "Usually, benifits of JIT become visible under composition of functions or large inputs. Let's try to implement a part of gradient descent optimizer with L2 regularization. Namely, we want to optimizer function\n",
    "$$\n",
    "    p_{n + 1} = p_{n} - u_{n},\n",
    "$$\n",
    "where optimization parameters are $p$ and $u$ is a final update to parameters which is calculated as\n",
    "$$\n",
    "    u_{n} = \\alpha \\cdot (g_n + \\lambda * p_{n}).\n",
    "$$\n",
    "Gradient of origin function is $g_n$, learning rate is $\\alpha$ and L2 regularizer is $\\lambda$.\n",
    "\n",
    "**NOTE** See `optax` library which implements optimizers in similar way in JAX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a796bd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(params, updates, learning_rate):\n",
    "    return learning_rate * updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "c64547e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted(params, updates, l2):\n",
    "    return params + l2 * updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "b0949a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sgd(learning_rate=1e-1, l2=1e-5):\n",
    "    if l2 == 0.0:\n",
    "        return partial(sgd, learning_rate=learning_rate)\n",
    "    \n",
    "    def fn(params, updates):\n",
    "        updates = weighted(params, updates, l2)\n",
    "        updates = sgd(params, updates, learning_rate)\n",
    "        return updates\n",
    "    \n",
    "    return fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "076fc33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_fn = make_sgd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "a3a28ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = np.random.normal(size=(1000, ))\n",
    "updates = np.random.normal(size=(1000, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "8a11d9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.93 µs ± 110 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit opt_fn(params, updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "9c253c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_fn_jit = jax.jit(opt_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "7b75f430",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = jnp.array(params)\n",
    "updates = jnp.array(updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "26e72ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.73 µs ± 72 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit opt_fn_jit(params, updates).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e4d1f0",
   "metadata": {},
   "source": [
    "### 2 Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0667e0",
   "metadata": {},
   "source": [
    "Symbolical differentiation is a wide-spreaded feature of numerical frameworks. JAX has advanced functionality to evaluate gradients of functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "1d133c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    return (params ** 2).sum()\n",
    "\n",
    "\n",
    "def objective_grad(params):\n",
    "    return 2 * params.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "5652bdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = jnp.array([1, 2], jnp.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "af37cfba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(5., dtype=float32)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objective(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "7087091a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([2., 4.], dtype=float32)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.grad(objective)(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "d29e62ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(6., dtype=float32)"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objective_grad(xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57157977",
   "metadata": {},
   "source": [
    "There is a \"shortcut\" to evaluate function value and its gradients at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "73667204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value: 5.0\n",
      "grad: [2. 4.]\n"
     ]
    }
   ],
   "source": [
    "value, grad = jax.value_and_grad(objective)(xs)\n",
    "print('value:', value)\n",
    "print('grad:', grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f878a33d",
   "metadata": {},
   "source": [
    "The second-order derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "ad0c12d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[2., 0.],\n",
       "             [0., 2.]], dtype=float32)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.jacobian(jax.grad(objective))(xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9c3826",
   "metadata": {},
   "source": [
    "And the thirdorder derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "2d8aa0ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[[0., 0.],\n",
       "              [0., 0.]],\n",
       "\n",
       "             [[0., 0.],\n",
       "              [0., 0.]]], dtype=float32)"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.jacobian(jax.jacobian(jax.grad(objective)))(xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3398e2ef",
   "metadata": {},
   "source": [
    "And compile the resulting function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "618dd6f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[[0., 0.],\n",
       "              [0., 0.]],\n",
       "\n",
       "             [[0., 0.],\n",
       "              [0., 0.]]], dtype=float32)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.jit(jax.jacobian(jax.jacobian(jax.grad(objective))))(xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eeb16a7",
   "metadata": {},
   "source": [
    "Also, JAX have routines for evaluation Jacobian-vector product `jax.jvp` and vector-Jacobian product `jax.vjp`. The latter is well-known as a backpropagation algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07345775",
   "metadata": {},
   "source": [
    "### 3 Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5e37ae",
   "metadata": {},
   "source": [
    "Vectorization transformation is a JAX variant of the classical one `numpy.vectorize`. The idea is to apply a function to a tensor along some axis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30537c5",
   "metadata": {},
   "source": [
    " In this trivial example we will find a sum of matrix rows and columns with built-in `numpy.sum` and `jax.vmap`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "7dc3dcc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "             [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "             [0., 0., 2., 0., 0., 0., 0., 0., 0., 0.],\n",
       "             [0., 0., 0., 3., 0., 0., 0., 0., 0., 0.],\n",
       "             [0., 0., 0., 0., 4., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat = jnp.diag(jnp.arange(10.))[:5]\n",
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "fbeb7fdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([0., 1., 2., 3., 4., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "62d342ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([0., 1., 2., 3., 4.], dtype=float32)"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.vmap(lambda x: x.sum(), in_axes=(0,))(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "7d63d884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([0., 1., 2., 3., 4.], dtype=float32)"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "09645a12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([0., 1., 2., 3., 4., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.vmap(lambda x: x.sum(), in_axes=(1,))(mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dd062d",
   "metadata": {},
   "source": [
    "Now, we can implement `jax.jacobian` as folows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "1901fc80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([2., 4.], dtype=float32)"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.grad(objective)(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "90ad421b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(x)>"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.vmap(lambda x: jax.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96a649b",
   "metadata": {},
   "source": [
    "### 4 Checkify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4934b11",
   "metadata": {},
   "source": [
    "JAX does not allow to throw exceptions in JITed function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "a6517da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def double(value):\n",
    "    value = jnp.asarray(value)\n",
    "    if (value < 0).any():\n",
    "        raise ValueError('Expected a scalar.')\n",
    "    return 2 * value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "2f2d9ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.jit(double)(1.0)  # Error: Throw ConcretizationTypeError exception!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4868bcb5",
   "metadata": {},
   "source": [
    "But there is an experimental feature `checkify` to check assertion on function arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "0bc322f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.experimental.checkify import check, checkify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "cd77b5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def double(value):\n",
    "    value = jnp.asarray(value)\n",
    "    check((value >= 0).any(), 'Expected a non-negative values.')\n",
    "    return 2 * value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "02d9fcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "err, res = jax.jit(checkify(double))(1.0)\n",
    "err.throw()  # OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "5021fb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "err, res = jax.jit(checkify(double))(-1.0)\n",
    "err.throw()  # Error: ValueError: Expected a scalar!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f89dfd9",
   "metadata": {},
   "source": [
    "Transformation `checkify` wraps computation in `Error` monad what enables us to check assertions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585b0d98",
   "metadata": {},
   "source": [
    "### 6 Control Flow and Static Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "5d2fb4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "53e67085",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnums=(0, 1, 4))\n",
    "def minimize(obj, opt, params, max_iter=20, verbose=False):    \n",
    "    for i in range(max_iter):\n",
    "        value, updates = jax.value_and_grad(obj)(params)\n",
    "        params -= opt(params, updates)\n",
    "        if verbose:\n",
    "            jax.debug.print('[{i}] value={value}', i=i, value=value)\n",
    "    return value, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "069e439a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03649292141199112"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value, params = minimize(objective, opt_fn, jnp.ones(2))\n",
    "value.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "eec0fdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnums=(0, 1, 4))\n",
    "def minimize(obj, opt, params, max_iter=20, verbose=False):    \n",
    "    def body_fn(index, state):\n",
    "        _, params = state\n",
    "        value, updates = jax.value_and_grad(obj)(params)\n",
    "        params -= opt(params, updates)\n",
    "        if verbose:\n",
    "            jax.debug.print('[{i}] value={value}', i=i, value=value)\n",
    "        return value, params\n",
    "    value, params = jax.lax.fori_loop(0, max_iter, body_fn, (0.0, params))\n",
    "    return value, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "20e88992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03649292141199112"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value, params = minimize(objective, opt_fn, jnp.ones(2))\n",
    "value.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "595191e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnums=(0, 1, 4))\n",
    "def minimize(obj, opt, params, max_iter=20, verbose=False):    \n",
    "    def cond_fn(state):\n",
    "        value, _ = state\n",
    "        jax.debug.print('[{i}] value={value}', i=1, value=value)\n",
    "        return value >= 1e-1\n",
    "    \n",
    "    def body_fn(state):\n",
    "        _, params = state\n",
    "        value, updates = jax.value_and_grad(obj)(params)\n",
    "        params -= opt(params, updates)\n",
    "        if verbose:\n",
    "            jax.debug.print('[{i}] value={value}', i=i, value=value)\n",
    "        return value, params\n",
    "    \n",
    "    value, params = jax.lax.while_loop(cond_fn, body_fn, (float('+inf'), params))\n",
    "    return value, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "0cb228e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] value=inf\n",
      "[1] value=2.0\n",
      "[1] value=1.6199928522109985\n",
      "[1] value=1.3121883869171143\n",
      "[1] value=1.0628678798675537\n",
      "[1] value=0.860919177532196\n",
      "[1] value=0.6973415017127991\n",
      "[1] value=0.5648440718650818\n",
      "[1] value=0.4575216770172119\n",
      "[1] value=0.3705909252166748\n",
      "[1] value=0.30017727613449097\n",
      "[1] value=0.2431425005197525\n",
      "[1] value=0.19694454967975616\n",
      "[1] value=0.15952438116073608\n",
      "[1] value=0.12921416759490967\n",
      "[1] value=0.10466301441192627\n",
      "[1] value=0.08477666229009628\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.08477666229009628"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value, params = minimize(objective, opt_fn, jnp.ones(2))\n",
    "value.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c92495",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(minimize.lower(objective, opt_fn, jnp.ones(2)).as_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42be307c",
   "metadata": {},
   "source": [
    "### 7 Tree Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7764c111",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56fb0d35",
   "metadata": {},
   "source": [
    "### 8 Common Gotchas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2aaa223",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f1f489a",
   "metadata": {},
   "source": [
    "### 9 Debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a58e85",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb316e49",
   "metadata": {},
   "source": [
    "### 10 Custom Types in JAX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffa8405",
   "metadata": {},
   "source": [
    "Sometimes we want to write high-level code without messy indexing. In this case we ussually define types. The issue is that user-defined types causes fails in JAX transformation by default."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4013078b",
   "metadata": {},
   "source": [
    "Consider canonical decomposition which can be generalized to CANDECOM/PARAFAC or simply CP in case of arbitrary dimension $d$.\n",
    "\n",
    "$$\n",
    "    A = U \\cdot V^T\n",
    "$$\n",
    "\n",
    "We want to calculate an element of matrix $A$ represented in CP-format. The calculation reduces to the following in the Einstein notation.\n",
    "\n",
    "$$\n",
    "    A_{ij} = U_{ik} V{jk}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0dc784dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e362bb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CP:\n",
    "    \n",
    "    cores: tuple[np.ndarray, ...]\n",
    "        \n",
    "    shape: tuple[int, ...]\n",
    "        \n",
    "    rank: int\n",
    "        \n",
    "    @property\n",
    "    def ndim(self) -> int:\n",
    "        return len(self.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2e4269a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = CP(cores=[jnp.ones((3, 2)), jnp.ones((3, 2))],\n",
    "        shape=(3, 3),\n",
    "        rank=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3bdc64b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getitem(cp: CP, index):\n",
    "    ix, jx = index\n",
    "    row = cp.cores[0][ix]\n",
    "    col = cp.cores[1][jx]\n",
    "    return row @ col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1e056c03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(2., dtype=float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getitem(cp, (0, 0))  # OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "094e72e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Argument 'CP(cores=[DeviceArray([[1., 1.],\n             [1., 1.],\n             [1., 1.]], dtype=float32), DeviceArray([[1., 1.],\n             [1., 1.],\n             [1., 1.]], dtype=float32)], shape=(3, 3), rank=2)' of type <class '__main__.CP'> is not a valid JAX type.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [64], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgetitem\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[0;32m/usr/lib/python3.10/site-packages/jax/_src/api.py:3016\u001b[0m, in \u001b[0;36m_check_arg\u001b[0;34m(arg)\u001b[0m\n\u001b[1;32m   3014\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_arg\u001b[39m(arg):\n\u001b[1;32m   3015\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(arg, core\u001b[38;5;241m.\u001b[39mTracer) \u001b[38;5;129;01mor\u001b[39;00m _valid_jaxtype(arg)):\n\u001b[0;32m-> 3016\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(arg)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid JAX type.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Argument 'CP(cores=[DeviceArray([[1., 1.],\n             [1., 1.],\n             [1., 1.]], dtype=float32), DeviceArray([[1., 1.],\n             [1., 1.],\n             [1., 1.]], dtype=float32)], shape=(3, 3), rank=2)' of type <class '__main__.CP'> is not a valid JAX type."
     ]
    }
   ],
   "source": [
    "jax.jit(getitem)(cp, (0, 0))  # Error: throws TypeError exception!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97290917",
   "metadata": {},
   "source": [
    "JAX works with trees (pytrees). So, we need say JAX how to serialize our data structure to tree and deserialize it form tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0df80400",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.tree_util import register_pytree_node_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6c5a1342",
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_pytree_node_class\n",
    "class Canonical:\n",
    "    \"\"\"Class Canonical is a container for tensor represented with\n",
    "    CP-decomposition.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cores: list[jax.Array], shape: list[int, ...],\n",
    "                 rank: int):\n",
    "        self.shape = tuple(shape)\n",
    "        self.rank = rank\n",
    "        self.cores = cores\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        params = f'ndim={self.ndim}, shape={self.shape}, ranks={self.rank}'\n",
    "        return f'{self.__class__.__name__}({params})'\n",
    "\n",
    "    @property\n",
    "    def ndim(self) -> int:\n",
    "        return len(self.shape)\n",
    "    \n",
    "    def tree_flatten(self):\n",
    "        return self.cores, {'shape': self.shape, 'rank': self.rank}\n",
    "\n",
    "    @classmethod\n",
    "    def tree_unflatten(cls, treedef, leaves):\n",
    "        return cls(leaves, **treedef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9be6c533",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = Canonical(cores=[jnp.ones((3, 2)), jnp.ones((3, 2))],\n",
    "               shape=(3, 3),\n",
    "               rank=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a827d455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(2., dtype=float32)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.jit(getitem)(cp, (0, 0))  # OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ab571e31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(2., dtype=float32)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.jit(getitem)(cp, (0, 0))  # OK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12349248",
   "metadata": {},
   "source": [
    "Why? Wrapped function with `jax.jit` performs ser/de to tree and from tree under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ff9edf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaves, treedef = jax.tree_util.tree_flatten(cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "50c7de65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Canonical(ndim=2, shape=(3, 3), ranks=2)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.tree_util.tree_unflatten(treedef, leaves)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ddfdbf",
   "metadata": {},
   "source": [
    "### 11 Custom Primitives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6720f3c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python39264bit0753bc0ebc15419dbfc48632fd59bdf7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
