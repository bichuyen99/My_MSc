{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {
    "id": "Ysau5wx0bpuA"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MaFsV50rXUT9"
   },
   "source": [
    "# Functions for Informer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {
    "id": "5e3KFXWrOyyV"
   },
   "outputs": [],
   "source": [
    "class GaussianBasisFunctions(object):\n",
    "    \"\"\"Function phi(t) = Gaussian(t; mu, sigma_sq).\"\"\"\n",
    "    def __init__(self, mu, sigma):\n",
    "        self.mu = mu.unsqueeze(0)\n",
    "        self.sigma = sigma.unsqueeze(0)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"GaussianBasisFunction(mu={self.mu}, sigma={self.sigma})\"\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of basis functions.\"\"\"\n",
    "        return self.mu.size(1)\n",
    "\n",
    "    def _phi(self, t):\n",
    "        # N(t|0,1)\n",
    "        return 1. / math.sqrt(2 * math.pi) * torch.exp(-.5 * t**2)\n",
    "\n",
    "    def _Phi(self, t):\n",
    "        return .5 * (1 + torch.erf(t / math.sqrt(2)))\n",
    "\n",
    "    def _integrate_product_of_gaussians(self, mu, sigma_sq):\n",
    "        sigma = torch.sqrt(self.sigma ** 2 + sigma_sq)\n",
    "        return self._phi((mu - self.mu) / sigma) / sigma\n",
    "\n",
    "    def evaluate(self, t):\n",
    "        # N(t|mu, sigma)\n",
    "        return self._phi((t - self.mu) / self.sigma) / self.sigma\n",
    "\n",
    "    def batch_evaluate(self, t):\n",
    "        t = t.repeat(self.mu.size(0),1) - self.mu.repeat(t.size(0),1).transpose(1,0)\n",
    "        return self._phi(t / self.sigma) / self.sigma\n",
    "\n",
    "    def integrate_t2_times_psi(self, a, b):\n",
    "        \"\"\"Compute integral int_a^b (t**2) * psi(t).\"\"\"\n",
    "        return (self.mu**2 + self.sigma**2) * (\n",
    "            self._Phi((b - self.mu) / self.sigma) - self._Phi((a - self.mu) / self.sigma)\n",
    "        ) - (\n",
    "            self.sigma * (b + self.mu) * self._phi((b - self.mu) / self.sigma)\n",
    "        ) + (\n",
    "            self.sigma * (a + self.mu) * self._phi((a - self.mu) / self.sigma)\n",
    "        )\n",
    "\n",
    "    def integrate_t_times_psi(self, a, b):\n",
    "        \"\"\"Compute integral int_a^b t * psi(t).\"\"\"\n",
    "        return self.mu * (\n",
    "            self._Phi((b - self.mu) / self.sigma) - self._Phi((a - self.mu) / self.sigma)\n",
    "        ) - self.sigma * (\n",
    "            self._phi((b - self.mu) / self.sigma) - self._phi((a - self.mu) / self.sigma)\n",
    "        )\n",
    "\n",
    "    def integrate_psi(self, a, b):\n",
    "        \"\"\"Compute integral int_a^b psi(t).\"\"\"\n",
    "        return self._Phi((b - self.mu) / self.sigma) - self._Phi((a - self.mu) / self.sigma)\n",
    "\n",
    "    def integrate_t2_times_psi_gaussian(self, mu, sigma_sq):\n",
    "        \"\"\"Compute integral int N(t; mu, sigma_sq) * t**2 * psi(t).\"\"\"\n",
    "        S_tilde = self._integrate_product_of_gaussians(mu, sigma_sq)\n",
    "        mu_tilde = (\n",
    "            self.mu * sigma_sq + mu * self.sigma ** 2\n",
    "        ) / (\n",
    "            self.sigma ** 2 + sigma_sq\n",
    "        )\n",
    "        sigma_sq_tilde = ((self.sigma ** 2) * sigma_sq) / (self.sigma ** 2 + sigma_sq)\n",
    "        return S_tilde * (mu_tilde ** 2 + sigma_sq_tilde)\n",
    "\n",
    "    def integrate_t_times_psi_gaussian(self, mu, sigma_sq):\n",
    "        \"\"\"Compute integral int N(t; mu, sigma_sq) * t * psi(t).\"\"\"\n",
    "        S_tilde = self._integrate_product_of_gaussians(mu, sigma_sq)\n",
    "        mu_tilde = (\n",
    "            self.mu * sigma_sq + mu * self.sigma ** 2\n",
    "        ) / (\n",
    "            self.sigma ** 2 + sigma_sq\n",
    "        )\n",
    "        return S_tilde * mu_tilde\n",
    "\n",
    "    def integrate_psi_gaussian(self, mu, sigma_sq):\n",
    "        \"\"\"Compute integral int N(t; mu, sigma_sq) * psi(t).\"\"\"\n",
    "        return self._integrate_product_of_gaussians(mu, sigma_sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {
    "id": "rqEteXJ0bi-q"
   },
   "outputs": [],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "class ContinuousSoftmaxFunction(torch.autograd.Function):\n",
    "\n",
    "    @classmethod\n",
    "    def _expectation_phi_psi(cls, ctx, mu, sigma_sq):\n",
    "        \"\"\"Compute expectation of phi(t) * psi(t).T under N(mu, sigma_sq).\"\"\"\n",
    "        num_basis = [len(basis_functions) for basis_functions in ctx.psi]\n",
    "        total_basis = sum(num_basis)\n",
    "        V = torch.zeros((mu.shape[0], 2, total_basis), dtype=ctx.dtype,device=ctx.device)\n",
    "        offsets = torch.cumsum(torch.IntTensor(num_basis).to(ctx.device), dim=0)\n",
    "        start = 0\n",
    "        for j, basis_functions in enumerate(ctx.psi):\n",
    "            V[:, 0, start:offsets[j]] = basis_functions.integrate_t_times_psi_gaussian(mu, sigma_sq)\n",
    "            V[:, 1, start:offsets[j]] = basis_functions.integrate_t2_times_psi_gaussian(mu, sigma_sq)\n",
    "            start = offsets[j]\n",
    "        return V\n",
    "\n",
    "    @classmethod\n",
    "    def _expectation_psi(cls, ctx, mu, sigma_sq):\n",
    "        \"\"\"Compute expectation of psi under N(mu, sigma_sq).\"\"\"\n",
    "        num_basis = [len(basis_functions) for basis_functions in ctx.psi]\n",
    "        total_basis = sum(num_basis)\n",
    "        r = torch.zeros(mu.shape[0], total_basis, dtype=ctx.dtype, device=ctx.device)\n",
    "        offsets = torch.cumsum(torch.IntTensor(num_basis).to(ctx.device), dim=0)\n",
    "        start = 0\n",
    "        for j, basis_functions in enumerate(ctx.psi):\n",
    "            r[:, start:offsets[j]] = basis_functions.integrate_psi_gaussian(mu, sigma_sq)\n",
    "            start = offsets[j]\n",
    "        return r\n",
    "\n",
    "    @classmethod\n",
    "    def _expectation_phi(cls, ctx, mu, sigma_sq):\n",
    "        \"\"\"Compute expectation of phi under N(mu, sigma_sq).\"\"\"\n",
    "        v = torch.zeros(mu.shape[0], 2, dtype=ctx.dtype, device=ctx.device)\n",
    "        v[:, 0] = mu.squeeze(1)\n",
    "        v[:, 1] = (mu**2 + sigma_sq).squeeze(1)\n",
    "        return v\n",
    "\n",
    "    @classmethod\n",
    "    def forward(cls, ctx, theta, psi):\n",
    "        # We assume a Gaussian.\n",
    "        # We have:\n",
    "        # theta = [mu/sigma**2, -1/(2*sigma**2)],\n",
    "        # phi(t) = [t, t**2],\n",
    "        # p(t) = Gaussian(t; mu, sigma**2).\n",
    "        ctx.dtype = theta.dtype\n",
    "        ctx.device = theta.device\n",
    "        ctx.psi = psi\n",
    "        sigma_sq = (-.5 / theta[:, 1]).unsqueeze(1)\n",
    "        mu = theta[:, 0].unsqueeze(1) * sigma_sq\n",
    "        r = cls._expectation_psi(ctx, mu, sigma_sq)\n",
    "        ctx.save_for_backward(mu, sigma_sq, r)\n",
    "        return r\n",
    "\n",
    "    @classmethod\n",
    "    def backward(cls, ctx, grad_output):\n",
    "        mu, sigma_sq, r = ctx.saved_tensors\n",
    "        J = cls._expectation_phi_psi(ctx, mu, sigma_sq)\n",
    "        e_phi = cls._expectation_phi(ctx, mu, sigma_sq)\n",
    "        e_psi = cls._expectation_psi(ctx, mu, sigma_sq)\n",
    "        J -= torch.bmm(e_phi.unsqueeze(2), e_psi.unsqueeze(1))\n",
    "        grad_input = torch.matmul(J, grad_output.unsqueeze(2)).squeeze(2)\n",
    "        return grad_input, None\n",
    "\n",
    "\n",
    "class ContinuousSoftmax(nn.Module):\n",
    "    def __init__(self, psi=None):\n",
    "        super(ContinuousSoftmax, self).__init__()\n",
    "        self.psi = psi\n",
    "\n",
    "    def forward(self, theta):\n",
    "        return ContinuousSoftmaxFunction.apply(theta, self.psi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exVGoDdWXZCD"
   },
   "source": [
    "# Informer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {
    "id": "U8zm0SmYeXc8"
   },
   "outputs": [],
   "source": [
    "class LongTermAttention(nn.Module):\n",
    "    # main class to compute continuous attention, with unbounded memory and sticky memories\n",
    "    # like all 3rd part of article is in this class\n",
    "    def __init__(self, \n",
    "                 head_size: int, \n",
    "                 length: int, \n",
    "                 attn_num_basis: int, # number of basis functions\n",
    "                 attn_drop: float, \n",
    "                 n_heads: int, \n",
    "                 d_model: int, # embeding size \n",
    "                 sigma_0 = 1., \n",
    "                 mu_0 = 0., \n",
    "                 **kwargs):\n",
    "\n",
    "        super(LongTermAttention, self).__init__()\n",
    "\n",
    "        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "        self.length = length                   # memory length\n",
    "        self.head_size = head_size\n",
    "        self.attn_num_basis = attn_num_basis   # N - num of basis functions\n",
    "        self.n_head = n_heads                  # number of heads\n",
    "        self.nb_samples = 512                  # number of samples (from past) used for update \n",
    "        self.tau = 0.75                        # compressing factor \n",
    "        self.ridge_penalty = 1                 # ridge penalty\n",
    "        self.sigma_0 = sigma_0\n",
    "        self.mu_0 = mu_0\n",
    "\n",
    "        self.B_past = None # previous coefficient matrix\n",
    "\n",
    "        padding = True \n",
    "        \n",
    "        self.proj_query = nn.Linear(n_heads * head_size, n_heads * head_size, bias=False)\n",
    "        self.proj_key   = nn.Linear(n_heads * head_size, n_heads * head_size, bias=False)\n",
    "        self.proj_value = nn.Linear(n_heads * head_size, n_heads * head_size, bias=False)\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(attn_drop)\n",
    "        self.attn_out = nn.Linear(n_heads * head_size, d_model, bias=False)\n",
    "        self.mu = nn.Linear(attn_num_basis, 1, bias=False)\n",
    "        self.sigma = nn.Linear(attn_num_basis, 1, bias=False)\n",
    "        self.softplus = torch.nn.Softplus()\n",
    "        self.mask_net = torch.nn.Conv1d(n_heads * head_size, n_heads * head_size, 3, padding=1)\n",
    "        self.transform = ContinuousSoftmax(psi=None)\n",
    "        \n",
    "        # get basis functions psi\n",
    "        sigmas = [0.01, 0.05] # basis function sigmas \n",
    "                              # maybe also need to be changed, \n",
    "                              # this parameters was taken from original repository\n",
    "        if attn_num_basis % len(sigmas):\n",
    "            attn_num_basis += (len(sigmas) - attn_num_basis % len(sigmas)) \n",
    "            \n",
    "        lengths  = []\n",
    "        for l in range(length):\n",
    "            lengths.append(l+1)\n",
    "       \n",
    "        self.psi = [self.get_gaussian_basis_functions(attn_num_basis, sigmas, device=self.device)]\n",
    "        \n",
    "        #========================== G ========================\n",
    "        # computation of G = F.T @ (F @ F.T + ridge_penalty * I)^(-1)\n",
    "        def compute_G(l, psi, positions):\n",
    "            # G = F.T @ (F @ F.T + ridge_penalty * I)^(-1)\n",
    "            F = torch.zeros(self.attn_num_basis, positions.size(0)) # [N,L]\n",
    "            F[:, :] = psi.evaluate(positions.unsqueeze(1)).t() # [N,L]\n",
    "            I = torch.eye(self.attn_num_basis) # [N,N]\n",
    "            G = F.t().matmul((F.matmul(F.t()) + self.ridge_penalty * I).inverse()) # actual formula #[L,N]\n",
    "            return G.to(self.device)\n",
    "        \n",
    "        # compute G \n",
    "        self.Gs  = []\n",
    "        for l in lengths:\n",
    "            simple_positions = []\n",
    "            for i in range(l):\n",
    "                simple_positions.append((i+1) / l)\n",
    "            simple_positions = torch.tensor(simple_positions).to(self.device)\n",
    "            self.Gs.append(compute_G(l, self.psi[0], simple_positions))\n",
    "            \n",
    "        # compute G for the infinite case\n",
    "        self.Ginf = []\n",
    "        for l in lengths:\n",
    "            inf_positions = []\n",
    "            tm_tau = torch.arange(1, self.nb_samples + 1).float()\n",
    "            tm_l = torch.arange(self.nb_samples + 1, l + self.nb_samples + 1).float()\n",
    "            tm_tau = tm_tau * self.tau / self.nb_samples # positions of old vectors\n",
    "            tm_l = self.tau + (1 - self.tau) * (tm_l - self.nb_samples) / length # positions of new vectors\n",
    "            positions_inf = torch.cat([tm_tau, tm_l],0).to(self.device)\n",
    "            self.Ginf.append(compute_G(l, self.psi[0], positions_inf))\n",
    "            \n",
    "        #======================== end G ==================== \n",
    "        \n",
    "        samples = None\n",
    "        tm_tau = torch.arange(1, self.nb_samples + 1).float()\n",
    "        tm_l = torch.arange(self.nb_samples + 1, l + self.nb_samples + 1).float()\n",
    "        tm_tau = tm_tau * self.tau / self.nb_samples\n",
    "        for t in tm_tau:\n",
    "            if samples is None:\n",
    "                samples = self.psi[0].evaluate(t/self.tau)\n",
    "            else:\n",
    "                samples = torch.cat([samples,self.psi[0].evaluate(t/self.tau)], dim=0)\n",
    "        self.samples = samples\n",
    "\n",
    "    def get_gaussian_basis_functions(self, nb_basis, sigmas, device):\n",
    "        mu, sigma = torch.meshgrid(torch.linspace(0, 1, nb_basis // len(sigmas)), torch.Tensor(sigmas))\n",
    "        mu = mu.flatten().to(device)\n",
    "        sigma = sigma.flatten().to(device)\n",
    "        assert mu.size(0) == nb_basis\n",
    "        return GaussianBasisFunctions(mu=mu, sigma=sigma)\n",
    "\n",
    "    def score(self, query, keys):\n",
    "        query = query / (self.d_head ** 0.5) # divide by sqrt(d_head) [B,h,q,d]\n",
    "        keys = keys.transpose(-1, -2)        #[B,h,d,N]\n",
    "        scores = torch.matmul(query, keys)   #[B,h,q,N] \n",
    "        return scores\n",
    "\n",
    "    def value_function(self, x, inf = False):\n",
    "        #x : [B, e, L]\n",
    "        if inf:\n",
    "            G = self.Ginf[x.size(-1)- 1 - self.nb_samples] # [nb_sample + L, N]\n",
    "        else:\n",
    "            G = self.Gs[x.size(-1) - 1] # [L, N]\n",
    "\n",
    "        B = torch.matmul(x, G)      # [B, e, N]\n",
    "        B = B.permute(0,2,1)        # [B, N, e]\n",
    "        return B\n",
    "\n",
    "    def update_inf(self, x):\n",
    "        l = x.shape[-1]\n",
    "        if self.B_past is not None:       \n",
    "            xm_tau = self.B_past.transpose(-1,-2).matmul(self.samples.transpose(0,1)) # [B,e,nb_samples]\n",
    "            x = torch.cat([xm_tau,x], dim=2)       # [B, e, nb_samples + L]\n",
    "            B = self.value_function(x, inf = True) # [B, N, e]\n",
    "        else:\n",
    "            B = self.value_function(x) # [B, N, e]\n",
    "        self.B_past = B.detach()\n",
    "        return B\n",
    "\n",
    "    def forward(self, k, q, new_doc = False, reg_mask = None):  \n",
    "        # k, q: [L, batch size, emb]\n",
    "        batch_size = k.size(1)       #batch size\n",
    "        qlen = q.size(0)             #query length\n",
    "        klen = k.size(0)             #key length\n",
    "        self.d_head = self.head_size #head size\n",
    "\n",
    "        # clean memory if going through different document\n",
    "        if new_doc:\n",
    "            self.B_past = None \n",
    "            \n",
    "        k = k.permute(1,2,0) # [B,e,L]\n",
    "        reg_mask = torch.sigmoid(self.mask_net(k))  \n",
    "        k = k * reg_mask\n",
    "\n",
    "        # perform memory update\n",
    "        B = self.update_inf(k)\n",
    "        \n",
    "        query  = q.permute(1,0,2)\n",
    "        keys   = self.proj_key(B)\n",
    "        values = self.proj_value(B)\n",
    "\n",
    "        query  = query.view(batch_size, qlen, self.n_head, self.d_head).transpose(1,2)                 # [B,h,q,d]\n",
    "        keys   = keys.view(batch_size, self.attn_num_basis, self.n_head, self.d_head).transpose(1,2)   # [B,h,N,d]\n",
    "        values = values.view(batch_size, self.attn_num_basis, self.n_head, self.d_head).transpose(1,2) # [B,h,N,d]\n",
    "        \n",
    "        #compute scores\n",
    "        scores = self.score(query, keys) #[B,h,q,N] \n",
    "\n",
    "        #computing mu and sigma\n",
    "        mu = torch.sigmoid(self.mu(scores)) #[B,h,q] \n",
    "        sigma_sq = self.softplus(self.sigma(scores))#[B,h,q] \n",
    "        mu = mu.view(-1)\n",
    "        sigma_sq = torch.clamp(sigma_sq, min=1e-6).view(-1)\n",
    "\n",
    "        sigma_0_sq = self.sigma_0**2\n",
    "        # computing kl_loss\n",
    "        if self.mu_0 is None:\n",
    "            kl_reg = 1/2 * (sigma_sq.view(batch_size, -1) / sigma_0_sq - \n",
    "                        torch.log(sigma_sq.view(batch_size, -1) / sigma_0_sq) - 1)\n",
    "        else:\n",
    "            kl_reg = 1/2 * (sigma_sq.view(batch_size, -1) / sigma_0_sq - \n",
    "                            torch.log(sigma_sq.view(batch_size,-1) / sigma_0_sq) - 1 +\n",
    "                            (mu.view(batch_size,-1) - self.mu_0) ** 2 / sigma_0_sq )\n",
    "        theta = torch.zeros(batch_size * self.n_head * qlen, 2, device = self.device)  # [B*h*q, 2]\n",
    "        theta[:, 0] = mu / sigma_sq\n",
    "        theta[:, 1] = -1. / (2. * sigma_sq)\n",
    "\n",
    "        # get basis functions\n",
    "        self.transform.psi = self.psi\n",
    "\n",
    "        #compute basis functions expectation\n",
    "        r = self.transform(theta) # [B*h*q,N] \n",
    "        r = r.view(batch_size, self.n_head, qlen, self.attn_num_basis).permute(0,1,3,2) # [B,h,N,q]\n",
    "\n",
    "        values = values.transpose(-1,-2)   # [B,h,d,N]\n",
    "        context = torch.matmul(values,r)   # [B,h,d,q]\n",
    "        context = context.permute(3,0,1,2) # [q,B,h,d]\n",
    "        context = context.contiguous().view(qlen, batch_size, self.n_head * self.d_head) # [q,B,e]\n",
    "        context = self.attn_out(context) # the Long Term Memory (LTM) representation     # [q,B,d_model]\n",
    "        \n",
    "        return context, kl_reg\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3R9BLKD4182n",
    "outputId": "ff8f2f5c-4d15-454a-d1fb-7f58246ee72d"
   },
   "outputs": [],
   "source": [
    "hidden_size=45\n",
    "num_heads=4\n",
    "num_layers=1\n",
    "head_size = 15\n",
    "L = 1000\n",
    "bs = 512\n",
    "dim_feedforward = 4 * hidden_size\n",
    "lt_att = LongTermAttention(head_size = head_size,\n",
    "                 length =  L,\n",
    "                 attn_num_basis = 100, \n",
    "                 attn_drop = 0.1, \n",
    "                 n_heads = num_heads, \n",
    "                 d_model = hidden_size,\n",
    "                 sigma_0 = 0,\n",
    "                 mu_0 = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sOiJnUuH4dUT",
    "outputId": "dc8a480e-2542-476f-dd28-bf417384f6ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:45.095000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1000, 512, 45]), torch.Size([512, 4000]))"
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = torch.rand(L, bs, head_size*num_heads)\n",
    "k = torch.rand(L, bs, head_size*num_heads)\n",
    "\n",
    "start_time = datetime.now()\n",
    "z, kl_loss = lt_att(q,k)\n",
    "print(datetime.now() - start_time)\n",
    "z.shape, kl_loss.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JTgYklH-XIxV"
   },
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JOtDqdoE970x",
    "outputId": "76e381f2-ecf6-49b9-dd26-9438fd9b9ecd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'REPO_NAME = \"MSDProject\"\\n!if [ -d {REPO_NAME} ]; then rm -Rf {REPO_NAME}; fi\\n!git clone https://github.com/NonameUntitled/{REPO_NAME}.git\\n!cd {REPO_NAME}'"
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"REPO_NAME = \"MSDProject\"\n",
    "!if [ -d {REPO_NAME} ]; then rm -Rf {REPO_NAME}; fi\n",
    "!git clone https://github.com/NonameUntitled/{REPO_NAME}.git\n",
    "!cd {REPO_NAME}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {
    "id": "NT57kmJkCWtM"
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "class MetaParent(type):\n",
    "\n",
    "    def __init__(cls, name, base, params, **kwargs):\n",
    "        super().__init__(name, base, params)\n",
    "        is_base_class = cls.mro()[1] is object\n",
    "        if is_base_class:\n",
    "            base_class = cls\n",
    "        else:\n",
    "            base_class_found = False\n",
    "            for key in cls.mro():\n",
    "                if isinstance(key, MetaParent) and key.mro()[1] is object:\n",
    "                    assert base_class_found is False, 'multiple base classes(bug)'\n",
    "                    base_class = key\n",
    "                    base_class_found = True\n",
    "            assert base_class_found is True, f'no base class for {name}'\n",
    "\n",
    "        if is_base_class:\n",
    "            cls._subclasses = {}\n",
    "\n",
    "        @classmethod\n",
    "        def __init_subclass__(scls, config_name=None):\n",
    "            super().__init_subclass__()\n",
    "            if config_name is not None:\n",
    "                if config_name in base_class._subclasses:\n",
    "                    raise ValueError(\"Class with name `{}` is already registered\".format(config_name))\n",
    "                base_class._subclasses[config_name] = scls\n",
    "\n",
    "        cls.__init_subclass__ = __init_subclass__\n",
    "\n",
    "        @classmethod\n",
    "        def parent_create_from_config(cls, config, **kwargs):\n",
    "            if 'type' in config:\n",
    "                return cls._subclasses[config['type']].create_from_config(config, **kwargs)\n",
    "            else:\n",
    "                raise ValueError('There is no `type` provided for the `{}` class'.format(name))\n",
    "\n",
    "        # Take kwargs for the last initialized baseclass\n",
    "        init_kwargs = {}\n",
    "        for bcls in cls.mro()[:-1]:  # Look into all base classes except object\n",
    "            if '__init__' not in bcls.__dict__:\n",
    "                continue\n",
    "            init_kwargs = inspect.signature(bcls.__init__).parameters\n",
    "            break\n",
    "\n",
    "        @classmethod\n",
    "        def child_create_from_config(cls, config, **kwargs):\n",
    "            kwargs = {}\n",
    "            for key, argspec in init_kwargs.items():\n",
    "                if key == 'self':\n",
    "                    continue\n",
    "                value = config.get(key, argspec.default)\n",
    "                if value is inspect.Parameter.empty:\n",
    "                    msg = 'There is no value for `{}.__init__` required field `{}` in config `{}`'\n",
    "                    raise ValueError(msg.format(cls, key, config))\n",
    "                kwargs[key] = value\n",
    "            return cls(**kwargs)\n",
    "\n",
    "        if 'create_from_config' not in cls.__dict__:\n",
    "            cls.create_from_config = parent_create_from_config if is_base_class else child_create_from_config\n",
    "\n",
    "\n",
    "class BaseModel(metaclass=MetaParent):\n",
    "    pass\n",
    "\n",
    "\n",
    "class TorchModel(nn.Module, BaseModel):\n",
    "    pass\n",
    "\n",
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "class BaselineProjector(torch.nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            embedding_dim,\n",
    "            num_types,\n",
    "            num_codes,\n",
    "            max_sequence_len,\n",
    "            use_positions=True,\n",
    "            use_log_amount=True,\n",
    "            use_layernorm=True,\n",
    "            eps=1e-5,\n",
    "            dropout=0.0,\n",
    "            aggregation_type='sum',\n",
    "            initializer_range=0.02\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self._embedding_dim = embedding_dim\n",
    "\n",
    "        self._num_types = num_types\n",
    "        self._num_codes = num_codes\n",
    "        self._max_sequence_len = max_sequence_len\n",
    "\n",
    "        self._use_positions = use_positions\n",
    "        self._use_log_amount = use_log_amount\n",
    "        self._use_layernorm = use_layernorm\n",
    "        self._eps = eps\n",
    "        self._dropout = dropout\n",
    "        self._aggregation_type = aggregation_type\n",
    "\n",
    "        self._types_embedding = nn.Embedding(\n",
    "            num_embeddings=self._num_types + 2,\n",
    "            embedding_dim=self._embedding_dim\n",
    "        )\n",
    "\n",
    "        self._codes_embedding = nn.Embedding(\n",
    "            num_embeddings=self._num_codes + 2,\n",
    "            embedding_dim=self._embedding_dim\n",
    "        )\n",
    "\n",
    "        self._amount_layer = nn.Linear(\n",
    "            in_features=1, out_features=self._embedding_dim\n",
    "        )\n",
    "\n",
    "        self._position_embedding = nn.Identity()\n",
    "        if self._use_positions:\n",
    "            self._position_embedding = nn.Embedding(\n",
    "                num_embeddings=self._max_sequence_len + 1,\n",
    "                embedding_dim=self._embedding_dim\n",
    "            )\n",
    "\n",
    "        self._dropout = nn.Dropout(p=self._dropout)\n",
    "        self._layernorm = nn.LayerNorm(self._embedding_dim, eps=self._eps)\n",
    "\n",
    "        self._init_weights(initializer_range)\n",
    "\n",
    "        self._output_dim = self._embedding_dim if self._aggregation_type == 'sum' \\\n",
    "            else (3 + self._use_positions) * self._embedding_dim\n",
    "\n",
    "    @classmethod\n",
    "    def create_from_config(cls, config, **kwargs):\n",
    "        return cls(\n",
    "            embedding_dim=config['embedding_dim'],\n",
    "            num_types=kwargs['num_types'],\n",
    "            num_codes=kwargs['num_codes'],\n",
    "            max_sequence_len=kwargs['max_sequence_len'],\n",
    "            use_positions=config.get('use_positions', True),\n",
    "            use_log_amount=config.get('use_log_amount', True),\n",
    "            use_layernorm=config.get('use_layernorm', True),\n",
    "            eps=config.get('eps', 1e-5),\n",
    "            dropout=config.get('dropout', 0.0),\n",
    "            aggregation_type=config.get('aggregation_type', 'sum'),\n",
    "            initializer_range=config.get('initializer_range', 0.02)\n",
    "        )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _init_weights(self, initializer_range):\n",
    "        nn.init.trunc_normal_(\n",
    "            self._types_embedding.weight.data,\n",
    "            std=initializer_range,\n",
    "            a=-2 * initializer_range,\n",
    "            b=2 * initializer_range\n",
    "        )\n",
    "\n",
    "        nn.init.uniform_(self._amount_layer.weight.data, a=initializer_range, b=initializer_range)\n",
    "        nn.init.uniform_(self._amount_layer.bias.data, a=initializer_range, b=initializer_range)\n",
    "\n",
    "        nn.init.trunc_normal_(\n",
    "            self._codes_embedding.weight.data,\n",
    "            std=initializer_range,\n",
    "            a=-2 * initializer_range,\n",
    "            b=2 * initializer_range\n",
    "        )\n",
    "\n",
    "        if self._use_positions:\n",
    "            nn.init.trunc_normal_(\n",
    "                self._position_embedding.weight.data,\n",
    "                std=initializer_range,\n",
    "                a=-2 * initializer_range,\n",
    "                b=2 * initializer_range\n",
    "            )\n",
    "\n",
    "        nn.init.ones_(self._layernorm.weight.data)\n",
    "        nn.init.zeros_(self._layernorm.bias.data)\n",
    "\n",
    "    @property\n",
    "    def output_dim(self):\n",
    "        return self._output_dim\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        mcc_code = inputs['mcc_code']  # (all_batch_events)\n",
    "        transaction_type = inputs['transaction_type']  # (all_batch_events)\n",
    "        amount = inputs['amount']  # (all_batch_events)\n",
    "\n",
    "        mcc_code_embeddings = self._dropout(self._codes_embedding(mcc_code))  # (all_batch_events, embedding_dim)\n",
    "        transaction_type_embeddings = self._dropout(self._types_embedding(transaction_type))  # (all_batch_events, embedding_dim)\n",
    "\n",
    "        if self._use_log_amount:\n",
    "            amount = torch.sign(amount) * torch.log(1. + torch.abs(amount))\n",
    "        amount_embeddings = self._dropout(self._amount_layer(amount.unsqueeze(-1)))\n",
    "\n",
    "        embeddings = [mcc_code_embeddings, transaction_type_embeddings, amount_embeddings]  # (3, all_batch_events, embedding_dim)\n",
    "\n",
    "        if self._use_positions:\n",
    "            position = inputs['positions']  # (all_batch_events)\n",
    "            position_embeddings = self._position_embedding(position)  # (all_batch_events, embedding_dim)\n",
    "            embeddings.append(position_embeddings)\n",
    "\n",
    "        if self._aggregation_type == 'sum':\n",
    "            embedding = None\n",
    "            for e in embeddings:\n",
    "                if embedding is None:\n",
    "                    embedding = e\n",
    "                else:\n",
    "                    embedding += e\n",
    "        else:\n",
    "            assert self._aggregation_type == 'concat'\n",
    "            embedding = torch.cat(embeddings, dim=0)\n",
    "\n",
    "        lengths = inputs['lengths']  # (batch_size)\n",
    "        batch_size = lengths.shape[0]\n",
    "        max_sequence_length = lengths.max().item()\n",
    "\n",
    "        padded_embeddings = torch.zeros(\n",
    "            batch_size, max_sequence_length,\n",
    "            self._embedding_dim if self._aggregation_type == 'sum' else self._embedding_dim * len(embeddings),\n",
    "            dtype=torch.float, device=DEVICE\n",
    "        )  # (batch_size, max_seq_len, emb_dim)\n",
    "\n",
    "        mask = torch.arange(\n",
    "            end=max_sequence_length,\n",
    "            device=DEVICE\n",
    "        )[None].tile([batch_size, 1]) < lengths[:, None]  # (batch_size, max_seq_len)\n",
    "\n",
    "        padded_embeddings[mask] = embedding\n",
    "        if self._use_layernorm:\n",
    "            padded_embeddings = self._layernorm(padded_embeddings)\n",
    "\n",
    "        return padded_embeddings, mask\n",
    "\n",
    "\n",
    "class BaselineEncoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            hidden_size,\n",
    "            num_heads,\n",
    "            num_layers,\n",
    "            dim_feedforward,\n",
    "            dropout=0.0,\n",
    "            activation='relu',\n",
    "            layer_norm_eps=1e-5,\n",
    "            input_dim=None,\n",
    "            output_dim=None,\n",
    "            user_cls_only=False,\n",
    "            initializer_range=0.02\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self._input_projection = nn.Identity()\n",
    "        if input_dim is not None:\n",
    "            self._input_projection = nn.Linear(input_dim, hidden_size)\n",
    "\n",
    "        transformer_encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_size,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            activation=get_activation_function(activation),\n",
    "            layer_norm_eps=layer_norm_eps,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self._encoder = nn.TransformerEncoder(transformer_encoder_layer, num_layers)\n",
    "\n",
    "        self._output_projection = nn.Identity()\n",
    "        if output_dim is not None:\n",
    "            self._output_projection = nn.Linear(hidden_size, output_dim)\n",
    "\n",
    "        self._user_cls_only = user_cls_only\n",
    "\n",
    "        self._init_weights(initializer_range)\n",
    "\n",
    "    @classmethod\n",
    "    def create_from_config(cls, config, **kwargs):\n",
    "        return cls(\n",
    "            hidden_size=config['hidden_size'],\n",
    "            num_heads=config['num_heads'],\n",
    "            num_layers=config['num_layers'],\n",
    "            dim_feedforward=config.get('dim_feedforward', 4 * config['hidden_size']),\n",
    "            dropout=config.get('dropout', 0.0),\n",
    "            activation=config.get('activation', 'relu'),\n",
    "            layer_norm_eps=config.get('layer_norm_eps', 1e-5),\n",
    "            input_dim=kwargs['input_dim'],\n",
    "            output_dim=config.get('output_dim', None),\n",
    "            user_cls_only=config.get('user_cls_only', False),\n",
    "            initializer_range=config.get('initializer_range', 0.02)\n",
    "        )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _init_weights(self, initializer_range):\n",
    "        for key, value in self.named_parameters():\n",
    "            if 'weight' in key:\n",
    "                if 'norm' in key:\n",
    "                    nn.init.ones_(value.data)\n",
    "                else:\n",
    "                    nn.init.trunc_normal_(\n",
    "                        value.data,\n",
    "                        std=initializer_range,\n",
    "                        a=-2 * initializer_range,\n",
    "                        b=2 * initializer_range\n",
    "                    )\n",
    "            elif 'bias' in key:\n",
    "                nn.init.zeros_(value.data)\n",
    "            else:\n",
    "                raise ValueError(f'Unknown transformer weight: {key}')\n",
    "\n",
    "    def forward(self, embeddings, attention_mask):\n",
    "        embeddings = self._input_projection(embeddings)  # (batch_size, seq_len, emb_dim)\n",
    "        embeddings = self._encoder(\n",
    "            src=embeddings,\n",
    "            src_key_padding_mask=~attention_mask\n",
    "        )  # (batch_size, seq_len, emb_dim)\n",
    "        embeddings = self._output_projection(embeddings)  # (batch_size, seq_len, output_emb_dim)\n",
    "        if self._user_cls_only:\n",
    "            embeddings = embeddings[:, 0, :]  # (batch_size, output_emb_dim)\n",
    "            attention_mask = attention_mask[:, 0]  # (batch_size)\n",
    "        return embeddings, attention_mask\n",
    "\n",
    "\n",
    "class BaselineModel(TorchModel, config_name='baseline'):\n",
    "\n",
    "    def __init__(self, projector, encoder):\n",
    "        super().__init__()\n",
    "        self._projector = projector\n",
    "        self._encoder = encoder\n",
    "\n",
    "    @classmethod\n",
    "    def create_from_config(cls, config, **kwargs):\n",
    "        projector = BaselineProjector.create_from_config(config['projector'], **kwargs)\n",
    "        encoder = BaselineEncoder.create_from_config(\n",
    "            config['encoder'],\n",
    "            input_dim=projector.output_dim,\n",
    "            **kwargs\n",
    "        )\n",
    "        return cls(projector, encoder)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeddings, mask = self._projector(inputs)\n",
    "        embeddings, mask = self._encoder(embeddings, mask)\n",
    "        return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {
    "id": "_yHmUwklGhvG"
   },
   "outputs": [],
   "source": [
    "def get_activation_function(name, **kwargs):\n",
    "    if name == 'relu':\n",
    "        return torch.nn.ReLU()\n",
    "    elif name == 'gelu':\n",
    "        return torch.nn.GELU()\n",
    "    elif name == 'elu':\n",
    "        return torch.nn.ELU(alpha=float(kwargs.get('alpha', 1.0)))\n",
    "    elif name == 'leaky':\n",
    "        return torch.nn.LeakyReLU(negative_slope=float(kwargs.get('negative_slope', 1e-2)))\n",
    "    elif name == 'sigmoid':\n",
    "        return torch.nn.Sigmoid()\n",
    "    elif name == 'tanh':\n",
    "        return torch.nn.Tanh()\n",
    "    elif name == 'softmax':\n",
    "        return torch.nn.Softmax()\n",
    "    elif name == 'softplus':\n",
    "        return torch.nn.Softplus(beta=int(kwargs.get('beta', 1.0)), threshold=int(kwargs.get('threshold', 20)))\n",
    "    elif name == 'softmax_logit':\n",
    "        return torch.nn.LogSoftmax()\n",
    "    else:\n",
    "        raise ValueError('Unknown activation function name `{}`'.format(name))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2TMEKSaiXOb0"
   },
   "source": [
    "# Test of speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {
    "id": "qD_BYey9AvfK"
   },
   "outputs": [],
   "source": [
    "hidden_size = 45\n",
    "num_heads = 4\n",
    "num_layers = 1\n",
    "head_size = 15\n",
    "L = 500\n",
    "bs = 512\n",
    "\n",
    "dim_feedforward = head_size * num_heads\n",
    "\n",
    "base = BaselineEncoder(hidden_size = head_size * num_heads, # hidden size need to be devidable by num_heads # embedings\n",
    "                num_heads = num_heads,\n",
    "                num_layers = num_layers,\n",
    "                dim_feedforward = head_size * num_heads)\n",
    "\n",
    "lt_att = LongTermAttention(head_size = head_size,\n",
    "                 length =  L,\n",
    "                 target_len =  1,\n",
    "                 attn_num_basis = 100, \n",
    "                 attn_drop = 0.1, \n",
    "                 n_heads = num_heads, \n",
    "                 d_model = 1 ,#for classification\n",
    "                 sigma_0 = 1.0,\n",
    "                 mu_0 = 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jc22aSIkG3UF",
    "outputId": "f0d12bf9-788a-4e43-9432-ba30717a94ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape torch.Size([512, 500, 60])\n",
      "mask.shape torch.Size([512, 500])\n",
      "0:00:14.278000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([512, 500, 60]), torch.Size([512, 500]))"
      ]
     },
     "execution_count": 553,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_dim = head_size * num_heads\n",
    "\n",
    "x = torch.rand(bs, L, head_size * num_heads) \n",
    "att = torch.rand(bs, L) > 0.5\n",
    "print(\"x.shape\", x.shape)\n",
    "print(\"mask.shape\", att.shape)\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "z, l = base(x, att)\n",
    "\n",
    "print(datetime.now() - start_time)\n",
    "\n",
    "z.shape, l.shape\n",
    "\n",
    "# Baseline [bs, L, hidden_size], [bs, L] -> [bs, L, hidden_size], [bs, L] #hidden_size = head_size * num_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1uiXGbRBVd9l",
    "outputId": "7fb3bb79-f6ba-4ce4-9edd-f9e83cc989a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape torch.Size([500, 512, 60])\n",
      "0:00:09.946000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([500, 512, 1]), torch.Size([512, 2000]))"
      ]
     },
     "execution_count": 554,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(L, bs, head_size * num_heads)\n",
    "print(\"x.shape\", x.shape)\n",
    "start_time = datetime.now()\n",
    "z, kl_loss = lt_att(x,x)\n",
    "print(datetime.now() - start_time)\n",
    "z.shape, kl_loss.shape\n",
    "\n",
    "# LongTermAttention [L, bs, head_size * num_head] x 2 -> [L, bs, d_model], [bs, L * num_heads]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nRCLyqmhPVX-"
   },
   "source": [
    "# Full Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {
    "id": "xuhrrxvgHr6_"
   },
   "outputs": [],
   "source": [
    "class BaselineModel(nn.Module):\n",
    "\n",
    "    def __init__(self, projector, encoder):\n",
    "        super().__init__()\n",
    "        self._projector = projector\n",
    "        self._encoder = encoder\n",
    "\n",
    "    \"\"\"@classmethod\n",
    "    def create_from_config(cls, config, **kwargs):\n",
    "        projector = BaselineProjector.create_from_config(config['projector'], **kwargs)\n",
    "        encoder = BaselineEncoder.create_from_config(\n",
    "            config['encoder'],\n",
    "            input_dim=projector.output_dim,\n",
    "            **kwargs\n",
    "        )\n",
    "        return cls(projector, encoder)\"\"\"\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeddings, mask = self._projector(inputs)\n",
    "        embeddings, mask = self._encoder(embeddings, mask)\n",
    "        return {\"predictions\": embeddings}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M9DyxbveHF5i"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S50NhWjq3Vd_"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {
    "id": "ZjGEGmOa3YA5"
   },
   "outputs": [],
   "source": [
    "class SimpleInformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 num_types,\n",
    "                 num_codes, \n",
    "                 max_sequence_len,\n",
    "                 num_heads, \n",
    "                 embedding_dim,\n",
    "                 n_heads, \n",
    "                 target_len = 70, \n",
    "                 attn_num_basis = 100,\n",
    "                 attn_drop = 0., \n",
    "                 sigma_0 = 1., \n",
    "                 mu_0 = 0.\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self._projector = BaselineProjector(embedding_dim = 32, \n",
    "                                            num_types = num_types, \n",
    "                                            num_codes = num_codes,\n",
    "                                            max_sequence_len = max_sequence_len)\n",
    "        assert embedding_dim % n_heads == 0\n",
    "        self._encoder = LongTermAttention(head_size = int(embedding_dim / n_heads),\n",
    "                                          length = max_sequence_len, \n",
    "                                          target_len = target_len, \n",
    "                                          attn_num_basis = attn_num_basis,\n",
    "                                          attn_drop = attn_drop, \n",
    "                                          n_heads = n_heads,\n",
    "                                          d_model = 1, \n",
    "                                          sigma_0 = sigma_0, \n",
    "                                          mu_0 = mu_0)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embeddings, mask = self._projector(inputs) # [bs, L, emb_dim]\n",
    "        embeddings = embeddings.permute(1,0,2) # [L, bs, emb_dim] \n",
    "        embeddings, kl_loss  = self._encoder(embeddings, embeddings) # [L, bs, d_model], [bs, L * num_heads] \n",
    "        embeddings = embeddings.permute(1,0,2) # [bs, L, d_model]\n",
    "        embeddings = embeddings[:, 0, :]\n",
    "        kl_loss = kl_loss.mean()\n",
    "\n",
    "        return {\"predictions\": embeddings, \"kl_loss\": kl_loss}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IE4idsHFFTo1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DYxXGmk8K-0k"
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {
    "id": "CTsgqq28K0NM"
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import logging\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import torch.utils.data\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "TRANSACTION_DATASET_NAME = 'transactions.bin'\n",
    "\n",
    "\n",
    "class BaseDataset:\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            path_to_transactions,\n",
    "            path_to_train_labels,\n",
    "            path_to_test_labels,\n",
    "            val_size=0.2,\n",
    "            min_history_len=None,\n",
    "            max_history_len=None\n",
    "    ):\n",
    "        self._path_to_transactions = path_to_transactions\n",
    "        self._path_to_train_labels = path_to_train_labels\n",
    "        self._path_to_test_labels = path_to_test_labels\n",
    "        self._val_size = val_size\n",
    "\n",
    "        self._min_history_len = min_history_len\n",
    "        self._max_history_len = max_history_len\n",
    "\n",
    "        # Dataset meta-information part\n",
    "        self._num_types = 0\n",
    "        self._num_codes = 0\n",
    "        self._max_sequence_len = 0\n",
    "        customer_id_mapping = {}\n",
    "\n",
    "        # Process all transactions\n",
    "        customers_history = defaultdict(list)\n",
    "\n",
    "        if not os.path.exists(TRANSACTION_DATASET_NAME):\n",
    "            transactions_dataset = pd.read_csv(self._path_to_transactions)\n",
    "            for _, row in tqdm(transactions_dataset.iterrows(), desc='Creating transactions dataset...'):\n",
    "                customer_id = int(row['customer_id'])\n",
    "                customer_id_fact = int(row['customer_id_factorized'])\n",
    "\n",
    "                customer_id_mapping[customer_id] = customer_id_fact\n",
    "\n",
    "                mcc_code = int(row['mcc_code_factorized'])\n",
    "                transaction_type = int(row['tr_type_factorized'])\n",
    "                amount = float(row['amount_log'])\n",
    "                timestamp = int(row['timestamp'])\n",
    "\n",
    "                customers_history[customer_id_fact].append({\n",
    "                    'mcc_code.idx': mcc_code,\n",
    "                    'transaction_type.idx': transaction_type,\n",
    "                    'amount.value': amount,\n",
    "                    'timestamp': timestamp\n",
    "                })\n",
    "\n",
    "                self._num_codes = max(self._num_codes, mcc_code)\n",
    "                self._num_types = max(self._num_types, transaction_type)\n",
    "\n",
    "            # Sort by timestamp\n",
    "            for customer_id, transactions in tqdm(customers_history.items(), desc='Sort all histories by timestamp...'):\n",
    "                customers_history[customer_id] = sorted(transactions, key=lambda x: x['timestamp'])\n",
    "                if self._max_history_len:\n",
    "                    customers_history[customer_id] = customers_history[customer_id][-self._max_history_len:]\n",
    "                self._max_sequence_len = max(self._max_sequence_len, len(customers_history[customer_id]))\n",
    "\n",
    "            status = (self._num_types, self._num_codes, self._max_sequence_len, customer_id_mapping, customers_history)\n",
    "\n",
    "            with open(TRANSACTION_DATASET_NAME, 'wb') as f:\n",
    "                pickle.dump(status, f)\n",
    "        else:\n",
    "            with open(TRANSACTION_DATASET_NAME, 'rb') as f:\n",
    "                status = pickle.load(f)\n",
    "\n",
    "            self._num_types, self._num_codes, self._max_sequence_len, customer_id_mapping, customers_history = status\n",
    "\n",
    "        self._customers_history = customers_history\n",
    "\n",
    "        # Train labels part\n",
    "        train_data = []\n",
    "        train_labels = pd.read_csv(self._path_to_train_labels)\n",
    "        for _, row in tqdm(train_labels.iterrows(), desc='Assigning train labels...'):\n",
    "            customer_id = int(row['customer_id'])\n",
    "            customer_id_fact = customer_id_mapping[customer_id]\n",
    "            label = int(row['gender'])\n",
    "\n",
    "            train_data.append({\n",
    "                'sample': customers_history[customer_id_fact],\n",
    "                'label': label\n",
    "            })\n",
    "\n",
    "        self._train_dataset = train_data\n",
    "\n",
    "        self._val_dataset = self._train_dataset[int(len(self._train_dataset) * (1 - self._val_size)):]\n",
    "        self._train_dataset = self._train_dataset[:int(len(self._train_dataset) * (1 - self._val_size))]\n",
    "\n",
    "        # Test labels part\n",
    "        test_data = []\n",
    "        test_labels = pd.read_csv(self._path_to_test_labels)\n",
    "        for _, row in tqdm(test_labels.iterrows(), desc='Assigning test labels...'):\n",
    "            customer_id = int(row['customer_id'])\n",
    "            customer_id_fact = customer_id_mapping[customer_id]\n",
    "            train_data.append({'sample': customers_history[customer_id_fact]})\n",
    "\n",
    "        self._test_dataset = test_data\n",
    "\n",
    "    @property\n",
    "    def num_types(self):\n",
    "        return self._num_types\n",
    "\n",
    "    @property\n",
    "    def num_codes(self):\n",
    "        return self._num_codes\n",
    "\n",
    "    @property\n",
    "    def max_sequence_len(self):\n",
    "        return self._max_sequence_len\n",
    "\n",
    "    @property\n",
    "    def datasets(self):\n",
    "        return self._train_dataset, self._val_dataset, self._test_dataset\n",
    "\n",
    "\n",
    "class BatchProcessor:\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        processed_batch = {}\n",
    "\n",
    "        processed_batch['lengths'] = []\n",
    "        for sample in batch:\n",
    "            processed_batch['lengths'].append(len(sample['sample']))\n",
    "        processed_batch['lengths'] = torch.tensor(processed_batch['lengths'], dtype=torch.long)\n",
    "\n",
    "        processed_batch['labels'] = []\n",
    "        for sample in batch:\n",
    "            processed_batch['labels'].append(sample['label'])\n",
    "        processed_batch['labels'] = torch.tensor(processed_batch['labels'], dtype=torch.long)\n",
    "\n",
    "        processed_batch['positions'] = []\n",
    "        for sample in batch:\n",
    "            processed_batch['positions'].extend(list(range(len(sample['sample']))))\n",
    "        processed_batch['positions'] = torch.tensor(processed_batch['positions'], dtype=torch.long)\n",
    "\n",
    "        for key in batch[0]['sample'][0].keys():\n",
    "            if key.endswith('.idx'):  # categorical feature\n",
    "                prefix = key.split('.')[0]\n",
    "                processed_batch[prefix] = []\n",
    "                for sample in batch:\n",
    "                    for event in sample['sample']:\n",
    "                        processed_batch[prefix].append(event[key])\n",
    "\n",
    "                processed_batch[prefix] = torch.tensor(processed_batch[prefix], dtype=torch.long)\n",
    "            elif key.endswith('.value'):  # scalar feature\n",
    "                prefix = key.split('.')[0]\n",
    "                processed_batch[prefix] = []\n",
    "                for sample in batch:\n",
    "                    for event in sample['sample']:\n",
    "                        processed_batch[prefix].append(event[key])\n",
    "\n",
    "                processed_batch[prefix] = torch.tensor(processed_batch[prefix], dtype=torch.float)\n",
    "            # else:\n",
    "            #     print(f'Unused feature: {key}')\n",
    "\n",
    "        return processed_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "fZFYYaFeFUbl",
    "outputId": "3b90f05d-47d4-4ad8-c9ee-c4fd77aab652"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Assigning train labels...: 8400it [00:02, 3329.37it/s]\n",
      "Assigning test labels...: 3600it [00:00, 4761.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset meta-information: num_types=76, num_codes=183, max_sequence_len=4000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path_to_transactions = os.path.join(\"\", 'transactions.csv')\n",
    "path_to_train_labels = os.path.join(\"\", 'gender_train.csv')\n",
    "path_to_test_labels = os.path.join(\"\", 'gender_test_kaggle_sample_submission.csv')\n",
    "\n",
    "dataset = BaseDataset(\n",
    "        path_to_transactions=path_to_transactions,\n",
    "        path_to_train_labels=path_to_train_labels,\n",
    "        path_to_test_labels=path_to_test_labels,\n",
    "        val_size=0.2,\n",
    "        max_history_len=2000  # !!!Important!!!\n",
    "    )\n",
    "\n",
    "print(f'Dataset meta-information: num_types={dataset.num_types}, '\n",
    "          f'num_codes={dataset.num_codes}, '\n",
    "          f'max_sequence_len={dataset.max_sequence_len}')\n",
    "\n",
    "train_dataset, val_dataset, _ = dataset.datasets\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = 8, shuffle= True, drop_last = True, collate_fn=BatchProcessor())\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, batch_size = 8, shuffle= True, drop_last = True, collate_fn=BatchProcessor())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O0k2fY07LFPb"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {
    "id": "mWwVSY1-HR6_"
   },
   "outputs": [],
   "source": [
    "\n",
    "def train(dataloader, model, optimizer, loss_function, epoch_cnt, alpha = 0.0):\n",
    "    step_num = 0\n",
    "    losses_per_steps = []\n",
    "    losses_per_epoches = []\n",
    "\n",
    "    print('Start training...')\n",
    "\n",
    "    for epoch in range(epoch_cnt):\n",
    "        tmp_losses = []\n",
    "        print(f'Start epoch {epoch}')\n",
    "        for step, inputs in enumerate(dataloader):\n",
    "            model.train()\n",
    "\n",
    "            for key, values in inputs.items():\n",
    "                inputs[key] = inputs[key].to(DEVICE)\n",
    "\n",
    "            predicts = model(inputs)\n",
    "            labels = inputs['labels'].float()\n",
    "            loss = loss_function(predicts[\"predictions\"].squeeze(), labels)\n",
    "            print(\"loss\", loss)\n",
    "            if \"kl_loss\" in predicts.keys():\n",
    "                loss += alpha * predicts[\"kl_loss\"]\n",
    "                print(\"kl_loss\", predicts[\"kl_loss\"])\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            #print('predicts', predicts, 'labels', labels, 'loss', loss)\n",
    "            print('loss', loss)\n",
    "            losses_per_steps.append(loss.detach().cpu().numpy())\n",
    "            tmp_losses.append(loss.detach().cpu().numpy())\n",
    "            step_num += 1\n",
    "            #print(losses_per_steps)\n",
    "            #print(np.mean(losses_per_steps))\n",
    "        losses_per_epoches.append(np.mean(tmp_losses))\n",
    "\n",
    "    print('Training procedure has been finished!')\n",
    "    return losses_per_epoches, losses_per_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lengths torch.Size([8])\n",
      "labels torch.Size([8])\n",
      "positions torch.Size([3776])\n",
      "mcc_code torch.Size([3776])\n",
      "transaction_type torch.Size([3776])\n",
      "amount torch.Size([3776])\n",
      "torch.Size([8, 1490, 32]) torch.Size([8, 1490])\n",
      "torch.Size([8, 1])\n",
      "torch.Size([8])\n",
      "torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "#some tests\n",
    "\n",
    "proj = BaselineProjector(32, num_types = 76, num_codes = 183, max_sequence_len = 4000)\n",
    "encoder = BaselineEncoder(input_dim = 32, output_dim = 1, hidden_size = 32, \n",
    "                          num_heads = 2, num_layers = 1, dim_feedforward = 6, \n",
    "                          user_cls_only = True)\n",
    "\n",
    "model = BaselineModel(proj, encoder)\n",
    "for dat in train_dataloader:\n",
    "    print(\"lengths\", dat[\"lengths\"].shape)\n",
    "    print(\"labels\", dat[\"labels\"].shape)\n",
    "    print(\"positions\", dat[\"positions\"].shape)\n",
    "    print(\"mcc_code\", dat[\"mcc_code\"].shape)\n",
    "    print(\"transaction_type\", dat[\"transaction_type\"].shape)\n",
    "    print(\"amount\", dat[\"amount\"].shape)\n",
    "    print(proj(dat)[0].shape, proj(dat)[1].shape)\n",
    "    print(encoder(proj(dat)[0], proj(dat)[1])[0].shape)\n",
    "    print(encoder(proj(dat)[0], proj(dat)[1])[1].shape)\n",
    "    print(model(dat)[\"predictions\"].squeeze().shape)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {
    "id": "e0KlvAAVSej5"
   },
   "outputs": [],
   "source": [
    "model = model.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_function = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {
    "id": "RMCar1ALSc9B"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "Start epoch 0\n",
      "loss tensor(0.6763, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6763, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6922, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6922, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6835, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6835, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6863, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6863, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6802, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6802, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6924, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6924, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6857, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6857, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6870, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6870, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6881, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6881, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6678, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6678, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6460, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6460, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7019, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7019, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7007, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7007, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6936, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6936, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6775, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6775, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7103, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7103, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6822, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6822, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6608, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6608, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6952, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6952, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6706, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6706, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7419, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7419, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6799, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6799, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6859, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6859, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6979, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6979, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7253, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7253, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6772, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6772, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6685, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6685, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6765, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6765, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6695, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6695, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6789, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6789, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7432, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7432, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6601, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6601, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7044, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7044, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7263, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7263, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6559, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6559, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6753, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6753, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6739, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6739, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6777, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6777, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7259, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7259, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7167, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7167, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7239, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7239, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7143, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7143, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6864, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6864, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6188, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6188, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6868, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6868, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6628, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6628, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6556, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6556, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6410, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6410, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6762, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6762, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7404, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7404, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6945, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6945, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6493, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6493, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7067, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7067, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6501, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6501, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6578, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6578, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7232, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7232, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6979, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6979, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7143, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7143, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7122, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7122, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6981, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tensor(0.6981, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6845, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6845, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7172, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7172, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6470, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6470, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6657, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6657, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7153, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7153, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6714, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6714, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7340, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7340, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6353, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6353, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6655, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6655, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6519, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6519, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6798, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6798, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6733, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6733, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6577, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6577, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6281, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6281, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7025, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7025, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6671, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6671, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7018, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7018, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6112, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6112, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7037, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7037, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6679, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6679, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6333, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6333, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6593, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6593, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6157, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6157, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6767, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6767, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6790, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6790, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6930, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6930, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7310, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7310, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6428, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6428, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7783, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7783, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7295, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7295, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6490, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6490, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6837, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6837, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7203, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7203, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6291, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6291, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7303, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7303, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7324, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7324, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6578, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6578, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6166, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6166, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6423, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6423, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6993, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6993, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6499, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6499, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7116, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7116, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5560, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5560, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6849, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6849, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6600, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6600, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7136, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7136, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6810, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6810, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6683, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6683, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6254, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6254, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7276, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7276, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6395, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6395, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7168, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7168, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6987, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6987, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6531, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6531, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7655, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7655, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7135, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7135, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6053, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6053, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6867, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6867, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6986, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6986, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tensor(0.7347, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7347, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7438, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7438, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6805, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6805, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6103, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6103, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6446, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6446, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7303, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7303, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6071, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6071, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5856, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5856, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6646, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6646, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6563, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6563, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7604, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7604, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7420, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7420, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7142, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7142, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7031, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7031, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7394, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7394, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6074, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6074, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6380, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6380, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7271, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7271, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7860, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7860, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6423, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6423, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7771, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7771, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7430, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7430, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6940, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6940, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7275, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7275, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6937, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6937, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7269, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7269, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7168, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7168, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6881, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6881, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7214, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7214, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6656, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6656, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7325, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7325, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6548, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6548, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7239, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7239, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6802, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6802, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6995, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6995, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6748, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6748, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6842, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6842, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7322, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7322, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7189, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7189, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7598, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7598, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6220, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6220, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6492, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6492, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6377, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6377, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6697, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6697, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6917, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6917, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7435, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7435, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7190, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7190, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6482, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6482, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6432, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6432, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6738, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6738, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6739, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6739, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6648, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6648, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6901, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6901, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7121, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7121, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6651, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6651, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6496, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6496, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6937, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6937, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6487, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6487, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6938, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6938, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6607, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tensor(0.6607, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6570, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6570, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6907, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6907, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7470, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7470, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6664, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6664, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6860, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6860, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6827, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6827, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6396, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6396, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7073, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7073, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6160, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6160, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6820, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6820, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7361, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7361, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7457, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7457, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6777, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6777, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7048, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7048, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6060, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6060, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6482, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6482, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7095, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7095, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6119, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6119, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6938, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6938, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6552, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6552, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7251, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7251, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7203, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7203, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6331, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6331, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6571, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6571, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7582, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7582, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7810, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7810, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6057, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6057, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6565, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6565, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7165, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7165, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6718, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6718, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6926, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6926, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Start epoch 1\n",
      "loss tensor(0.6528, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6528, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6419, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6419, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6779, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6779, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7187, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7187, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7094, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7094, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7300, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7300, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6373, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6373, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7227, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7227, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7032, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7032, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6355, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6355, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6637, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6637, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6578, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6578, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6822, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6822, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6846, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6846, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5950, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5950, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6804, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6804, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6604, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6604, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6377, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6377, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6689, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6689, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7285, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7285, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6375, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6375, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6638, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6638, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6666, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6666, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6664, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6664, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6213, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6213, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6878, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6878, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6794, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6794, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7510, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7510, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tensor(0.6768, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6768, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7016, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7016, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7184, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7184, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6372, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6372, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6647, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6647, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5941, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5941, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6215, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6215, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6311, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6311, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7101, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7101, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7112, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7112, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6973, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6973, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6523, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6523, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6813, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6813, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7150, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7150, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5973, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5973, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7075, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7075, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6404, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6404, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6321, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6321, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7296, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7296, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6584, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6584, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7125, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7125, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5948, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5948, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6930, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6930, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6458, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6458, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.8269, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.8269, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6001, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6001, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6892, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6892, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6286, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6286, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5635, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5635, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6604, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6604, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7373, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7373, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6721, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6721, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6855, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6855, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5514, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5514, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5562, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5562, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6672, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6672, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6398, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6398, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6370, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6370, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6437, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6437, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5390, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5390, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5471, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5471, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6967, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6967, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6845, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6845, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6816, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6816, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5945, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5945, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6451, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6451, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7284, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7284, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7668, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7668, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6589, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6589, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5985, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5985, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6126, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6126, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6254, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6254, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7082, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7082, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7333, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7333, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6537, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6537, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6312, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6312, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7363, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7363, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6631, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6631, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7483, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7483, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5923, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tensor(0.5923, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.8712, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.8712, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7089, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7089, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6523, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6523, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7206, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7206, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6675, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6675, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6454, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6454, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7474, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7474, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6508, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6508, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7289, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7289, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7506, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7506, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6536, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6536, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6163, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6163, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6943, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6943, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6791, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6791, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5712, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5712, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7295, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7295, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7288, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7288, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5981, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5981, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6916, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6916, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7488, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7488, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6968, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6968, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7986, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7986, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7522, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7522, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6150, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6150, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6385, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6385, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6765, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6765, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6801, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6801, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7711, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7711, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6447, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6447, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6829, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6829, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7787, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7787, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5867, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5867, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5733, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5733, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7323, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7323, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6512, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6512, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6001, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6001, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7014, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7014, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6142, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6142, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6974, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6974, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6684, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6684, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7066, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7066, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6480, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6480, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6865, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6865, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7034, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7034, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6996, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6996, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6721, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6721, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7830, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7830, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7303, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7303, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6949, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6949, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7215, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7215, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7239, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7239, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6355, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6355, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7868, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7868, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7175, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7175, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7575, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7575, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6496, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6496, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6682, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6682, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6675, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6675, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7408, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7408, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tensor(0.7624, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7624, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5979, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5979, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7330, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7330, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7353, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7353, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6815, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6815, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7116, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7116, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6758, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6758, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6945, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6945, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6969, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6969, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5706, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5706, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.8182, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.8182, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7247, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7247, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6270, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6270, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6654, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6654, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6514, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6514, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6677, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6677, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7027, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7027, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6875, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6875, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6942, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6942, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6441, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6441, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7139, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7139, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6197, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6197, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6604, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6604, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7047, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7047, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6695, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6695, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6585, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6585, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6811, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6811, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7401, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7401, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6450, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6450, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7070, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7070, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6773, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6773, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7711, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7711, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6743, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6743, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6727, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6727, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7171, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7171, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7189, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7189, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7178, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7178, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7350, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7350, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7158, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7158, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6274, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6274, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6486, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6486, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6654, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6654, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7114, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7114, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7505, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7505, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6305, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6305, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7525, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7525, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6574, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6574, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6448, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6448, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6454, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6454, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7163, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7163, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6534, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6534, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6109, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6109, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6540, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6540, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6469, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6469, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7015, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7015, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6661, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6661, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6796, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6796, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6401, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6401, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7263, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7263, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6629, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tensor(0.6629, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6738, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6738, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6853, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6853, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6486, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6486, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Start epoch 2\n",
      "loss tensor(0.7222, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7222, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6388, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6388, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7037, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7037, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6607, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6607, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7436, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7436, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7070, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7070, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7166, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7166, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6321, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6321, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6165, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6165, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6605, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6605, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6403, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6403, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7197, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7197, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7061, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7061, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6664, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6664, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6304, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6304, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6043, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6043, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6095, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6095, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6901, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6901, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6313, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6313, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6439, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6439, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6956, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6956, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6562, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6562, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6396, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6396, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7285, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7285, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6163, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6163, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7165, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7165, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6736, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6736, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6050, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6050, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5779, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5779, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6193, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6193, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5830, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5830, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6581, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6581, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6752, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6752, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6450, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6450, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6749, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6749, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6342, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6342, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.8344, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.8344, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6154, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6154, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.8259, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.8259, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6907, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6907, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6423, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6423, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7646, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7646, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7268, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7268, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6008, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6008, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7226, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7226, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7065, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7065, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5923, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5923, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7338, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7338, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6996, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6996, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7197, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7197, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6058, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6058, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7661, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7661, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7750, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7750, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7470, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7470, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6557, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6557, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6909, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6909, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tensor(0.6146, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6146, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6978, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6978, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7089, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7089, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6190, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6190, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5968, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5968, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6641, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6641, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6473, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6473, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6549, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6549, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5870, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5870, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7947, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7947, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6963, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6963, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7130, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7130, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6743, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6743, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7611, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7611, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7507, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7507, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7166, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7166, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6900, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6900, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6317, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6317, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7223, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7223, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6317, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6317, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6570, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6570, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5715, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5715, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7125, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7125, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6790, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6790, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6750, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6750, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6638, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6638, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6499, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6499, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6850, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6850, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6128, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6128, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7502, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7502, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6950, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6950, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6536, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6536, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6493, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6493, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5670, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5670, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6699, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6699, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5780, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5780, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6661, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6661, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6343, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6343, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6618, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6618, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5896, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5896, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7088, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7088, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6191, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6191, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7158, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7158, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7107, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7107, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7066, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7066, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6649, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6649, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6483, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6483, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7063, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7063, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6851, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6851, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7436, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7436, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7220, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7220, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6326, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6326, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6812, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6812, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5932, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5932, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7573, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7573, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6856, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6856, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6947, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6947, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7639, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7639, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7073, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7073, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6336, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tensor(0.6336, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7002, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7002, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7292, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7292, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6536, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6536, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6883, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6883, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6546, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6546, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7022, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7022, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6119, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6119, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6531, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6531, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6869, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6869, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6478, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6478, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7049, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7049, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6860, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6860, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7018, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7018, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5949, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5949, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6402, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6402, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6293, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6293, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6307, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6307, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6561, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6561, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6097, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6097, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6364, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6364, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6595, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6595, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6814, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6814, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6493, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6493, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6671, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6671, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6489, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6489, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7093, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7093, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7303, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7303, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7043, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7043, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7889, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7889, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6966, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6966, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6294, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6294, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6674, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6674, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7511, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7511, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6173, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6173, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7426, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7426, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7312, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7312, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5809, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5809, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7471, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7471, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7495, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7495, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6227, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6227, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6611, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6611, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7039, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7039, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6800, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6800, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5923, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5923, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6368, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6368, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6575, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6575, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6215, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6215, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6724, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6724, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7507, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7507, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6549, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6549, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6883, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6883, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7192, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7192, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6895, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6895, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7774, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7774, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6981, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6981, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7569, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7569, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6014, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6014, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6288, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6288, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6464, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6464, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tensor(0.7319, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7319, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6154, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6154, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6807, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6807, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6842, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6842, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6339, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6339, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5913, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5913, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6105, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6105, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6969, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6969, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6394, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6394, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7591, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7591, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7279, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7279, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.8370, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.8370, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7416, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7416, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6630, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6630, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6981, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6981, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6091, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6091, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7241, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7241, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.8143, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.8143, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5955, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5955, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6413, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6413, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6412, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6412, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6688, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6688, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5750, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5750, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5453, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5453, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6291, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6291, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7703, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7703, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7205, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7205, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7394, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7394, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7375, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7375, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5805, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.5805, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7215, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.7215, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6375, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6375, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6294, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6294, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6666, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6666, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6377, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "loss tensor(0.6377, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Training procedure has been finished!\n"
     ]
    }
   ],
   "source": [
    "# Train process\n",
    "losses_per_epoches, losses_per_steps = train(dataloader=val_dataloader, \n",
    "                                             model=model, optimizer=optimizer, \n",
    "                                             loss_function=loss_function, epoch_cnt= 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x305bc340>]"
      ]
     },
     "execution_count": 602,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# wow it takes much time\n",
    "plt.plot(losses_per_epoches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inf = SimpleInformerModel(num_types = 76, \n",
    "                                num_codes = 183, \n",
    "                                max_sequence_len = 4000,\n",
    "                                num_heads = 2, \n",
    "                                embedding_dim = 32, \n",
    "                                n_heads = 2, \n",
    "                                target_len = 1, \n",
    "                                attn_num_basis = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleInformerModel(\n",
       "  (_projector): BaselineProjector(\n",
       "    (_types_embedding): Embedding(78, 32)\n",
       "    (_codes_embedding): Embedding(185, 32)\n",
       "    (_amount_layer): Linear(in_features=1, out_features=32, bias=True)\n",
       "    (_position_embedding): Embedding(4001, 32)\n",
       "    (_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (_layernorm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (_encoder): LongTermAttention(\n",
       "    (proj_query): Linear(in_features=32, out_features=32, bias=False)\n",
       "    (proj_key): Linear(in_features=32, out_features=32, bias=False)\n",
       "    (proj_value): Linear(in_features=32, out_features=32, bias=False)\n",
       "    (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (attn_out): Linear(in_features=32, out_features=1, bias=False)\n",
       "    (mu): Linear(in_features=2, out_features=1, bias=False)\n",
       "    (sigma): Linear(in_features=2, out_features=1, bias=False)\n",
       "    (softplus): Softplus(beta=1, threshold=20)\n",
       "    (mask_net): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (transform): ContinuousSoftmax()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 604,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input length of seq tensor(1076)\n",
      "prediction torch.Size([8, 1])\n",
      "kl_loss torch.Size([])\n",
      "input length of seq tensor(1471)\n",
      "prediction torch.Size([8, 1])\n",
      "kl_loss torch.Size([])\n",
      "input length of seq tensor(1784)\n",
      "prediction torch.Size([8, 1])\n",
      "kl_loss torch.Size([])\n",
      "input length of seq tensor(999)\n",
      "prediction torch.Size([8, 1])\n",
      "kl_loss torch.Size([])\n",
      "input length of seq tensor(834)\n",
      "prediction torch.Size([8, 1])\n",
      "kl_loss torch.Size([])\n",
      "input length of seq tensor(1814)\n",
      "prediction torch.Size([8, 1])\n",
      "kl_loss torch.Size([])\n",
      "input length of seq tensor(792)\n",
      "prediction torch.Size([8, 1])\n",
      "kl_loss torch.Size([])\n",
      "input length of seq tensor(1017)\n",
      "prediction torch.Size([8, 1])\n",
      "kl_loss torch.Size([])\n",
      "input length of seq tensor(1308)\n",
      "prediction torch.Size([8, 1])\n",
      "kl_loss torch.Size([])\n",
      "input length of seq tensor(700)\n",
      "prediction torch.Size([8, 1])\n",
      "kl_loss torch.Size([])\n",
      "input length of seq tensor(692)\n",
      "prediction torch.Size([8, 1])\n",
      "kl_loss torch.Size([])\n",
      "input length of seq tensor(760)\n",
      "prediction torch.Size([8, 1])\n",
      "kl_loss torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "for i, dat in enumerate(train_dataloader):\n",
    "    print(\"input length of seq\", dat[\"lengths\"].max())\n",
    "    res = model_inf(dat)\n",
    "    print(\"prediction\", res[\"predictions\"].shape)\n",
    "    print(\"kl_loss\", res[\"kl_loss\"].shape)\n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "Start epoch 0\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6932, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6932, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7087, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6932, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6932, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6932, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6932, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6932, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6932, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6932, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6932, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6932, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "Start epoch 1\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "Start epoch 2\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "loss tensor(0.6931, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "kl_loss tensor(0.1548, grad_fn=<MeanBackward0>)\n",
      "loss tensor(0.7086, grad_fn=<AddBackward0>)\n",
      "Training procedure has been finished!\n"
     ]
    }
   ],
   "source": [
    "# Train process\n",
    "losses_per_epoches_inf, losses_per_steps_inf = train(dataloader=val_dataloader, \n",
    "                                             model=model_inf, optimizer=optimizer, \n",
    "                                             loss_function=loss_function, epoch_cnt= 3,\n",
    "                                            alpha = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x3371e070>]"
      ]
     },
     "execution_count": 607,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#it was much faster, but doesn't seem that it trains\n",
    "plt.plot(losses_per_epoches_inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7086301, 0.7086302, 0.7086302]"
      ]
     },
     "execution_count": 608,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses_per_epoches_inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "It works faster but, doesn't seem to train, maybe we need to change hyperparameters. Or maybe we need to do like in article: sum representations from Long Term attention and regular one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
