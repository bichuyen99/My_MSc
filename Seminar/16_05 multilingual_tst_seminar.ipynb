{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last time we have encountered the task of text detoxification for the English language. Let's recall how it looks like. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://ibb.co/GtqSwd3\"><img src=\"https://i.ibb.co/hZSz5g1/image.png\" alt=\"image\" border=\"0\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, parallel data is not often available in different languages. Multilingual LLMs ([mBART](https://arxiv.org/abs/2001.08210), [mT5](https://arxiv.org/abs/2010.11934), [mT0](https://arxiv.org/abs/2211.01786) etc.) come and help. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this seminar we will review some of the methods for cross-lingual and multilingual textual style transfer. \n",
    "\n",
    "So, if we are facing any of the sequence-to-sequence tasks in a particular language, but there is no data in that language, there are several solutions to this problem:\n",
    " \n",
    "    1. Translate data that you have to the language you need and use multilingual language model. \n",
    "    2. Use Backtranslation approach:\n",
    "        - Take the data on the language you want your model to work.\n",
    "        - Translate to the language on which the model is available\n",
    "        - Do TST.\n",
    "        - Translate the result back to the original language. \n",
    "    3. Use Adapters etc. \n",
    "\n",
    "\n",
    "Today we will cover the first approach and we will show some modifications of it. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backtranslation approach:\n",
    "\n",
    "<a href=\"https://ibb.co/JHJ9R4k\"><img src=\"https://i.ibb.co/QMw4FRm/image.png\" alt=\"image\" border=\"0\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data translation approach:\n",
    "\n",
    "<a href=\"https://ibb.co/3dmK0bW\"><img src=\"https://i.ibb.co/T2MyHGR/image.png\" alt=\"image\" border=\"0\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['NVIDIA_VISIBLE_DEVICES']='3'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='3'\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    MBartForConditionalGeneration,\n",
    "    MBart50TokenizerFast,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gdown --id 12g9yfO8hykt1JH7gfKre55BRokln1bEj\n",
    "# !gdown --id 1Tii3nAOvTgkwvBkIChgv-NZy_njZIqEa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "en2ru_google = pd.read_csv('../data/detox_en2ru_google.tsv')\n",
    "en2ru_yandex = pd.read_csv('../data/english_data/detox_en2ru_yandex.tsv', sep='\\t')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case we have taken ParaDetox dataset and translated it with Google Translate and Yandex Translate (both translators show approximately same performance.)\n",
    "\n",
    "Let's take a look at the data we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic_comment</th>\n",
       "      <th>neutral_comment</th>\n",
       "      <th>toxic_ru</th>\n",
       "      <th>neutral_ru</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>he had steel balls too !</td>\n",
       "      <td>he was brave too!</td>\n",
       "      <td>у него тоже были стальные яйца!</td>\n",
       "      <td>он тоже был смелым!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dude should have been taken to api , he would ...</td>\n",
       "      <td>It would have been good if he went to api. He ...</td>\n",
       "      <td>чувака надо было отвести в апи, он был бы как ...</td>\n",
       "      <td>Было бы хорошо, если бы он пошел на апи. Он бы...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im not gonna sell the fucking picture , i just...</td>\n",
       "      <td>I'm not gonna sell the picture, i just want to...</td>\n",
       "      <td>Я не собираюсь продавать эту чертову картинку,...</td>\n",
       "      <td>Я не собираюсь продавать картинку, я просто хо...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the garbage that is being created by cnn and o...</td>\n",
       "      <td>the news that is being created by cnn and othe...</td>\n",
       "      <td>мусор, создаваемый CNN и другими информационны...</td>\n",
       "      <td>новости, создаваемые CNN и другими информацион...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the reason they dont exist is because neither ...</td>\n",
       "      <td>The reason they don't exist is because neither...</td>\n",
       "      <td>причина, по которой их не существует, заключае...</td>\n",
       "      <td>Причина, по которой они не существуют, заключа...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       toxic_comment  \\\n",
       "0                           he had steel balls too !   \n",
       "1  dude should have been taken to api , he would ...   \n",
       "2  im not gonna sell the fucking picture , i just...   \n",
       "3  the garbage that is being created by cnn and o...   \n",
       "4  the reason they dont exist is because neither ...   \n",
       "\n",
       "                                     neutral_comment  \\\n",
       "0                                  he was brave too!   \n",
       "1  It would have been good if he went to api. He ...   \n",
       "2  I'm not gonna sell the picture, i just want to...   \n",
       "3  the news that is being created by cnn and othe...   \n",
       "4  The reason they don't exist is because neither...   \n",
       "\n",
       "                                            toxic_ru  \\\n",
       "0                    у него тоже были стальные яйца!   \n",
       "1  чувака надо было отвести в апи, он был бы как ...   \n",
       "2  Я не собираюсь продавать эту чертову картинку,...   \n",
       "3  мусор, создаваемый CNN и другими информационны...   \n",
       "4  причина, по которой их не существует, заключае...   \n",
       "\n",
       "                                          neutral_ru  \n",
       "0                                он тоже был смелым!  \n",
       "1  Было бы хорошо, если бы он пошел на апи. Он бы...  \n",
       "2  Я не собираюсь продавать картинку, я просто хо...  \n",
       "3  новости, создаваемые CNN и другими информацион...  \n",
       "4  Причина, по которой они не существуют, заключа...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en2ru_google.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic_comment</th>\n",
       "      <th>toxic_ru</th>\n",
       "      <th>neutral_comment</th>\n",
       "      <th>neutral_ru</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>he had steel balls too !</td>\n",
       "      <td>у него тоже были стальные яйца!</td>\n",
       "      <td>he was brave too!</td>\n",
       "      <td>он тоже был храбрым!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dude should have been taken to api , he would ...</td>\n",
       "      <td>чувака надо было отвезти в апи, он был бы там ...</td>\n",
       "      <td>It would have been good if he went to api. He ...</td>\n",
       "      <td>Было бы хорошо, если бы он пошел в апи. Он бы ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im not gonna sell the fucking picture , i just...</td>\n",
       "      <td>я не собираюсь продавать эту гребаную фотограф...</td>\n",
       "      <td>I'm not gonna sell the picture, i just want to...</td>\n",
       "      <td>Я не собираюсь продавать фотографию, я просто ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the garbage that is being created by cnn and o...</td>\n",
       "      <td>мусор, который создают cnn и другие информацио...</td>\n",
       "      <td>the news that is being created by cnn and othe...</td>\n",
       "      <td>новости, которые создают cnn и другие информац...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the reason they dont exist is because neither ...</td>\n",
       "      <td>причина, по которой их не существует, заключае...</td>\n",
       "      <td>The reason they don't exist is because neither...</td>\n",
       "      <td>Причина, по которой их не существует, заключае...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       toxic_comment  \\\n",
       "0                           he had steel balls too !   \n",
       "1  dude should have been taken to api , he would ...   \n",
       "2  im not gonna sell the fucking picture , i just...   \n",
       "3  the garbage that is being created by cnn and o...   \n",
       "4  the reason they dont exist is because neither ...   \n",
       "\n",
       "                                            toxic_ru  \\\n",
       "0                    у него тоже были стальные яйца!   \n",
       "1  чувака надо было отвезти в апи, он был бы там ...   \n",
       "2  я не собираюсь продавать эту гребаную фотограф...   \n",
       "3  мусор, который создают cnn и другие информацио...   \n",
       "4  причина, по которой их не существует, заключае...   \n",
       "\n",
       "                                     neutral_comment  \\\n",
       "0                                  he was brave too!   \n",
       "1  It would have been good if he went to api. He ...   \n",
       "2  I'm not gonna sell the picture, i just want to...   \n",
       "3  the news that is being created by cnn and othe...   \n",
       "4  The reason they don't exist is because neither...   \n",
       "\n",
       "                                          neutral_ru  \n",
       "0                               он тоже был храбрым!  \n",
       "1  Было бы хорошо, если бы он пошел в апи. Он бы ...  \n",
       "2  Я не собираюсь продавать фотографию, я просто ...  \n",
       "3  новости, которые создают cnn и другие информац...  \n",
       "4  Причина, по которой их не существует, заключае...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en2ru_yandex[['toxic_comment', 'toxic_ru', 'neutral_comment', 'neutral_ru']].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, we have several multilingual models like mBART and mT5 that have been trained on gigabytes of multilingual texts. \n",
    "\n",
    "These multilingual models can do the same detoxification taks but for multiple languages with still decent performance.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetoxDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        source = self.tokenizer(\n",
    "            self.data.iloc[idx].toxic_comment,\n",
    "            max_length=150,\n",
    "            pad_to_max_length=True,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        target = self.tokenizer(\n",
    "            self.data.iloc[idx].neutral_comment,\n",
    "            max_length=150,\n",
    "            pad_to_max_length=True,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        source[\"labels\"] = target[\"input_ids\"]\n",
    "\n",
    "        return {k: v.squeeze(0) for k, v in source.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will simply expand our data and concat original \"toxic\" -> \"polite\" part with the translated part. That gives us **twice** more training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ru_part = en2ru_yandex[['toxic_ru', 'neutral_ru']].copy()\n",
    "data = pd.concat(\n",
    "    [en2ru_yandex[['toxic_comment', 'neutral_comment']], \n",
    "    ru_part.rename(columns={'toxic_ru': 'toxic_comment', 'neutral_ru': 'neutral_comment'})]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50\").cuda()\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_part, valid_part = train_test_split(data, random_state=42, test_size=0.01)\n",
    "trainset = DetoxDataset(train_part, tokenizer)\n",
    "valset = DetoxDataset(valid_part, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"mbart_mdetox\",\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=1e-5,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"no\",\n",
    "    save_total_limit=1,\n",
    "    logging_steps=500,\n",
    "    gradient_accumulation_steps=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=trainset,\n",
    "    eval_dataset=valset,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/storage/moskovskiy/workspace/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 39136\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4892\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33metomoscow\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "2023-05-16 15:30:50.225355: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-05-16 15:30:50.225403: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.17"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/storage/moskovskiy/workspace/nlp/multilingual_detox/notebooks/wandb/run-20230516_153048-1vi19lrm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/etomoscow/huggingface/runs/1vi19lrm\" target=\"_blank\">mbart_mdetox</a></strong> to <a href=\"https://wandb.ai/etomoscow/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1001' max='4892' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1001/4892 07:14 < 28:11, 2.30 it/s, Epoch 0.20/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.552700</td>\n",
       "      <td>0.111055</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='33' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [33/50 00:03 < 00:02, 8.47 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 396\n",
      "  Batch size = 8\n",
      "  Num examples = 396\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 396\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 396\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 396\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 396\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 396\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 396\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 396\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 396\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 396\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4892, training_loss=0.3439985986430433, metrics={'train_runtime': 2161.436, 'train_samples_per_second': 18.106, 'train_steps_per_second': 2.263, 'total_flos': 1.24237486227456e+16, 'train_loss': 0.3439985986430433, 'epoch': 1.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is trained, let's try to generate something!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paraphrase(\n",
    "    text,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    n=None,\n",
    "    max_length=\"auto\",\n",
    "    beams=5,\n",
    "):\n",
    "    texts = [text] if isinstance(text, str) else text\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True)[\"input_ids\"].to(\n",
    "        model.device\n",
    "    )\n",
    "\n",
    "    if max_length == \"auto\":\n",
    "        max_length = inputs.shape[1] + 10\n",
    "\n",
    "    result = model.generate(\n",
    "        inputs,\n",
    "        num_return_sequences=n or 1,\n",
    "        do_sample=False,\n",
    "        temperature=1.0,\n",
    "        repetition_penalty=10.0,\n",
    "        max_length=max_length,\n",
    "        min_length=int(0.5 * max_length),\n",
    "        num_beams=beams,\n",
    "        # forced_bos_token_id=tokenizer.lang_code_to_id[tokenizer.tgt_lang],\n",
    "    )\n",
    "    texts = [tokenizer.decode(r, skip_special_tokens=True) for r in result]\n",
    "\n",
    "    return texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download some data for testing.\n",
    "\n",
    "# !gdown --id 1sVdnUUNa2k9CkQGrST5XnoqpOxR44hHW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('../data/russian_data/test.tsv', sep='\\t')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to check the performance of our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fdd2510d10f43f9aab2b6a029029c7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ReadTimeout), entering retry loop.\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm \n",
    "\n",
    "detox_ru = [\n",
    "    paraphrase(text, model, tokenizer)\n",
    "    for text in tqdm(test_data['toxic_comment'].values)\n",
    "]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see, what we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: ублюдок! закон бумеранга еще никто не отменял!!\n",
      "Generated: закон бумеранга еще никто не отменял!!\n",
      "Ground Truth: закон бумеранга еще никто не отменял!\n",
      "Source: Ах тыж ебаный ты нахуй! Как уснуть теперь\n",
      "Generated: Как уснуть в этот раз? Ты на самом деле плохой.\n",
      "Ground Truth: Ах тыж! Как уснуть теперь\n",
      "Source: не надо опускаться до уровня шута безмозглого\n",
      "Generated: не надо опускаться до уровня безмозглого.\n",
      "Ground Truth: Не надо опускаться до нижнего уровня\n",
      "Source: потому что жизнь долбоёбов ничему не учит\n",
      "Generated: Потому что жизнь людей ничему не учит.\n",
      "Ground Truth: потому что жизнь глупого ничему не учит\n"
     ]
    }
   ],
   "source": [
    "for index in [10, 12, 99, 93]:\n",
    "    print(f\"Source: {test_data['toxic_comment'][index]}\")\n",
    "    print(f\"Generated: {detox_ru[index]}\")\n",
    "    print(f\"Ground Truth: {test_data['neutral_comment'][index]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# from IPython.display import clear_output\n",
    "# sys.path.append('..')\n",
    "\n",
    "# from evaluate_ru import evaluate_style_transfer, load_model\n",
    "\n",
    "# style_model, style_tokenizer = load_model(\n",
    "#     \"IlyaGusev/rubertconv_toxic_clf\", use_cuda=True\n",
    "# )\n",
    "# meaning_model, meaning_tokenizer = load_model(\n",
    "#     \"s-nlp/rubert-base-cased-conversational-paraphrase-v1\", use_cuda=True\n",
    "# )\n",
    "# cola_model, cola_tolenizer = load_model(\n",
    "#     \"s-nlp/ruRoberta-large-RuCoLa-v1\", use_cuda=True\n",
    "# )\n",
    "\n",
    "# def evaluate(original, rewritten, references=None):\n",
    "#     return evaluate_style_transfer(\n",
    "#         original_texts=original,\n",
    "#         rewritten_texts=rewritten,\n",
    "#         references=references,\n",
    "#         style_model=style_model,\n",
    "#         style_tokenizer=style_tokenizer,\n",
    "#         meaning_model=meaning_model,\n",
    "#         meaning_tokenizer=meaning_tokenizer,\n",
    "#         cola_model=cola_model,\n",
    "#         cola_tokenizer=cola_tolenizer,\n",
    "#         style_target_label=0,\n",
    "#         meaning_target_label=0,\n",
    "#         cola_target_label=0,\n",
    "#         aggregate=True\n",
    "#         )\n",
    "\n",
    "# # refs = open('../data/english_data/test_neutral_parallel.txt', 'r').read().split('\\n')\n",
    "\n",
    "# results = evaluate(test_data['toxic_comment'].values, test_data['neutral_comment'].values, detox_ru)\n",
    "\n",
    "# clear_output()\n",
    "\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a311123ba0e54f099d8a716b75c1ea0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/671 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open('../data/english_data/test_toxic_parallel.txt', 'r') as f:\n",
    "    test_en = f.read().split('\\n')\n",
    "\n",
    "detox_en = [paraphrase(text, model, tokenizer) for text in tqdm(test_en)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.9500501155853271, 'similarity': 0.5612009167671204, 'fluency': 0.8355763554573059, 'joint': 0.4501223564147949, 'chrF': 68.79356510085162}\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from evaluate_en import evaluate_style_transfer, load_model\n",
    "\n",
    "style_model, style_tokenizer = load_model(\n",
    "    \"s-nlp/roberta_toxicity_classifier\", use_cuda=True\n",
    ")\n",
    "meaning_model, meaning_tokenizer = load_model(\n",
    "    \"Elron/bleurt-large-128\", use_cuda=True\n",
    ")\n",
    "cola_model, cola_tolenizer = load_model(\n",
    "    \"cointegrated/roberta-large-cola-krishna2020\", use_cuda=True\n",
    ")\n",
    "\n",
    "def evaluate(original, rewritten, references=None):\n",
    "    return evaluate_style_transfer(\n",
    "        original_texts=original,\n",
    "        rewritten_texts=rewritten,\n",
    "        references=references,\n",
    "        style_model=style_model,\n",
    "        style_tokenizer=style_tokenizer,\n",
    "        meaning_model=meaning_model,\n",
    "        meaning_tokenizer=meaning_tokenizer,\n",
    "        cola_model=cola_model,\n",
    "        cola_tokenizer=cola_tolenizer,\n",
    "        style_target_label=0,\n",
    "        meaning_target_label=0,\n",
    "        cola_target_label=0,\n",
    "        aggregate=True\n",
    "        )\n",
    "\n",
    "refs = open('../data/english_data/test_neutral_parallel.txt', 'r').read().split('\\n')\n",
    "\n",
    "results = evaluate(test_en, refs[:-1], detox_en)\n",
    "\n",
    "clear_output()\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: actually no , i hate looking for shit .\n",
      "Generated: actually no, i hate looking for things.\n",
      "Source: after all most of these dorks are millionares and can pay their own way .\n",
      "Generated: After all most of them are millionares and can pay their own way.\n",
      "Source: DIGIT years later and they still blame bush for his fuck ups .\n",
      "Generated: DIGIT years later and they still blame bush for his mistakes.\n",
      "Source: destined to repeat the same shit over and over again until we die .\n",
      "Generated: destined to repeat the same thing over and over again until we die.\n"
     ]
    }
   ],
   "source": [
    "for index in [10, 12, 99, 93]:\n",
    "    print(f\"Source: {test_en[index]}\")\n",
    "    print(f\"Generated: {detox_en[index]}\")\n",
    "    # print(f\"Ground Truth: {test_data['neutral_comment'][index]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data for training was multilingual, our model can perform detoxification on BOTH English and Russian."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-lingual approach"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not only can we train the model to be multilingual, but also cross-lingual. \n",
    "\n",
    "How? \n",
    "\n",
    "Simply pass the pairs of \"English Toxic Text\" ---> \"Russian Polite Text\" and you're good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ../saved_models/mbart_large_50_en_ru/checkpoint-40000/config.json\n",
      "Model config MBartConfig {\n",
      "  \"_name_or_path\": \"facebook/mbart-large-50\",\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": true,\n",
      "  \"architectures\": [\n",
      "    \"MBartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 200,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"mbart\",\n",
      "  \"normalize_before\": true,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 5,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"tokenizer_class\": \"MBart50Tokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250054\n",
      "}\n",
      "\n",
      "loading weights file ../saved_models/mbart_large_50_en_ru/checkpoint-40000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MBartForConditionalGeneration.\n",
      "\n",
      "All the weights of MBartForConditionalGeneration were initialized from the model checkpoint at ../saved_models/mbart_large_50_en_ru/checkpoint-40000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MBartForConditionalGeneration for predictions without further training.\n",
      "loading file https://huggingface.co/facebook/mbart-large-50/resolve/main/sentencepiece.bpe.model from cache at /home/moskovskiy/.cache/huggingface/transformers/67d3c795fa98268609962dbbd51f9c2a532a92638ad74e79dfed71791af90217.71e50b08dbe7e5375398e165096cacc3d2086119d6a449364490da6908de655e\n",
      "loading file https://huggingface.co/facebook/mbart-large-50/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/facebook/mbart-large-50/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/facebook/mbart-large-50/resolve/main/special_tokens_map.json from cache at /home/moskovskiy/.cache/huggingface/transformers/b84d4a0379a665c99c99c99a288e32cc76d92f6d30d0546b6c561c9c6f583946.ac77c0b56ab82aca841e254aa35803773ca3f42af7b173cc9e56af3bc76083d0\n",
      "loading file https://huggingface.co/facebook/mbart-large-50/resolve/main/tokenizer_config.json from cache at /home/moskovskiy/.cache/huggingface/transformers/daff1f01b888a47cdbe3d56766d2468e76d8d6cff9661b84ae0d57a3eb10732e.a94e252ee9791a9d77f2cef0e43c636de01335ae4b9d98087c92dc320b682dca\n",
      "loading configuration file https://huggingface.co/facebook/mbart-large-50/resolve/main/config.json from cache at /home/moskovskiy/.cache/huggingface/transformers/f05465e59eda7f301b62d284f0aff5987c4eafb42ddef0f71b9a8e4c4f6e00f2.b12b1c70d50e4d5e2fcb7773b69c8bbdd6d9f2e18e435c2b368caea016a3ef77\n",
      "Model config MBartConfig {\n",
      "  \"_name_or_path\": \"facebook/mbart-large-50\",\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": true,\n",
      "  \"architectures\": [\n",
      "    \"MBartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 200,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"mbart\",\n",
      "  \"normalize_before\": true,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 5,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"tokenizer_class\": \"MBart50Tokenizer\",\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250054\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/facebook/mbart-large-50/resolve/main/config.json from cache at /home/moskovskiy/.cache/huggingface/transformers/f05465e59eda7f301b62d284f0aff5987c4eafb42ddef0f71b9a8e4c4f6e00f2.b12b1c70d50e4d5e2fcb7773b69c8bbdd6d9f2e18e435c2b368caea016a3ef77\n",
      "Model config MBartConfig {\n",
      "  \"_name_or_path\": \"facebook/mbart-large-50\",\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": true,\n",
      "  \"architectures\": [\n",
      "    \"MBartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 200,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"mbart\",\n",
      "  \"normalize_before\": true,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 5,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"tokenizer_class\": \"MBart50Tokenizer\",\n",
      "  \"transformers_version\": \"4.21.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250054\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model2 = MBartForConditionalGeneration.from_pretrained('../saved_models/mbart_large_50_en_ru/checkpoint-40000')\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained('facebook/mbart-large-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96ed851759754be9b99b48d94c71534e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/671 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open('../data/english_data/test_toxic_parallel.txt', 'r') as f:\n",
    "    test_en = f.read().split('\\n')\n",
    "\n",
    "detox_en2ru = [paraphrase(text, model2.cuda(), tokenizer) for text in tqdm(test_en)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source:  mandated  and \" right fucking now \" would be good .\n",
      "Generated: mandated и \" прямо сейчас \" было бы хорошо.\n",
      "Source: &gt today was one of the most fucked up days of my life .\n",
      "Generated: Сегодня был один из самых трудных дней в моей жизни.\n",
      "Source: actually no , i hate looking for shit .\n",
      "Generated: На самом деле нет, я ненавижу искать что-то плохое.\n",
      "Source: all i got from that was shits gonna go down and nobody is going to be ready .\n",
      "Generated: Все, что я получил от этого, - это то, что произойдет, и никто не будет готов.\n",
      "Source: almost as fucked up as the cia funding and arming bin laden .\n",
      "Generated: почти так же плохо, как финансирование ЦРУ и вооружение Бен Ладена.\n"
     ]
    }
   ],
   "source": [
    "for i in [1, 5, 10, 15, 20]:\n",
    "    print(f\"Source: {test_en[i]}\")\n",
    "    print(f\"Generated: {detox_en2ru[i]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some advices for future. \n",
    "\n",
    "1. If you lack data for target language, use any other dataset available - multitask training **BOOSTS** performance of the whole model. As a proof, check out [mT0 paper](https://arxiv.org/abs/2211.01786).\n",
    "2. If the language you are working with is **very** rare, consider using Backtranslation approach. \n",
    "3. When working with translation problems, use either Google\\Yandex\\DeepL Translate or LLM like BLOOM. Please avoid using open-source translators like [opus-mt](https://huggingface.co/Helsinki-NLP/opus-mt-ru-en), they are much worse. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
