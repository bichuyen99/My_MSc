{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76cd698e-c76a-491c-8bf9-167b6e229036",
   "metadata": {},
   "source": [
    "# 07 Basics of Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b29945b-7614-4d28-ab32-b8ab9de96b2d",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "\n",
    "- Seminar is very introductory \n",
    "- No deep theory\n",
    "- Basic examples of main parts\n",
    "\n",
    "Ther are a lot of libraries to do deep learning in Python ([Pytorch](https://pytorch.org/tutorials/beginner/introyt/introyt1_tutorial.html), [TesnorFlow](https://www.tensorflow.org/learn), [JAX](https://jax.readthedocs.io/en/latest/notebooks/quickstart.html)). You may even do it with pure numpy. And even in Java or C++.\n",
    "\n",
    "In this example we will use PyTorch.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2a1593-4a85-4ff2-a69f-d812696be757",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import trange\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (11, 7)\n",
    "plt.rcParams['font.size'] = 20\n",
    "plt.rcParams['lines.linewidth'] = 3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806867ae-9e2c-4ab2-9a58-25cfb8b0a727",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron\n",
    "\n",
    "- First idea of Neural Networks by Frank Rosenblatt, 1958.\n",
    "\n",
    "- First appearance of ubiqutous and famous convolutional neural network (whatever it is) apper also in 1950s. Formalized in 1990.\n",
    "\n",
    "- Now neural networks (expecially large language models) are among most hot topics in the web.\n",
    "\n",
    "![Deep Feed-Forward Neural Network](media/mlp1.png)\n",
    "\n",
    "**What is this anyway?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ed9400-f6e1-4f98-8dc3-aeb374a6f108",
   "metadata": {},
   "source": [
    "### Lets start with linear regression\n",
    "\n",
    "Function we wish to approximate: $y_{true} = f^*(x)$\n",
    "\n",
    "Machine learning model: $\\hat{y} = f(x)\\, , \\, \\hat{y} \\approx y_{true}$ \n",
    "\n",
    "And if we use simple linear regression (with single feature):\n",
    "\n",
    "$$y = a x + b$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661ba2be-d3b0-427c-879c-647342a10ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 50, 20)\n",
    "noise = np.random.normal(0, 1.2, size=20) * 10\n",
    "y_true = 5 * x - 53\n",
    "y_noisy = (5 * x - 53) + noise\n",
    "\n",
    "plt.plot(x, y_true, 'r', label='Real function')\n",
    "plt.plot(x, y_noisy, 'o', c='b', label='Noisy observations')\n",
    "\n",
    "plt.xlabel(\"x\", fontsize=15)\n",
    "plt.ylabel('y', fontsize=15)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738ffd47-e63b-415a-bf3d-f8a5f3457218",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression(fit_intercept=True)\n",
    "lin_reg.fit(x.reshape(-1, 1), y_noisy)\n",
    "\n",
    "plt.plot(x, lin_reg.predict(x.reshape(-1, 1)), '--', c='g', label=\"ML model\")\n",
    "plt.plot(x, y_true, 'r', label='Real function')\n",
    "plt.plot(x, y_noisy, 'o', c='b', label='Noisy observations')\n",
    "\n",
    "plt.xlabel(\"x\", fontsize=15)\n",
    "plt.ylabel('y', fontsize=15)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b18b1b5-50d5-47e2-adde-4cc46cc73c51",
   "metadata": {},
   "source": [
    "### Now say we have $n$ features:\n",
    "\n",
    "$$y = a_1 \\cdot x_1 + a_2 \\cdot x_2 + \\dots + a_n \\cdot x_n + b \\, ,$$\n",
    "\n",
    "and in vector form:\n",
    "\n",
    "$$y = \\begin{bmatrix} a_1 & a_2 & \\cdots & a_n \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix} + b \\, .$$\n",
    "### Not single ouput?\n",
    "\n",
    "You may want to predict multiple outputs with your model (say, not only the price of a house, but also the income of a real estate agent). Sklearn in this case will fit [two separate models simultaneously](https://numpy.org/doc/stable/reference/generated/numpy.linalg.lstsq.html):\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "      y^1 & = & a_1^1 \\cdot x_1 + a_2^1 \\cdot x_2 + \\dots + a_n^1 \\cdot x_n + b^1 \\\\\n",
    "      y^2 & = & a_1^2 \\cdot x_1 + a_2^2 \\cdot x_2 + \\dots + a_n^2 \\cdot x_n + b^2\n",
    "    \\end{cases} \\, ,\n",
    "$$\n",
    "\n",
    "with $k$ targets:\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "      y^1 & = & a_1^1 \\cdot x_1 + a_2^1 \\cdot x_2 + \\dots + a_n^1 \\cdot x_n + b^1 \\\\\n",
    "      y^2 & = & a_1^2 \\cdot x_1 + a_2^2 \\cdot x_2 + \\dots + a_n^2 \\cdot x_n + b^2 \\\\\n",
    "      & \\cdots &  \\\\\n",
    "      y^k & = & a_1^k \\cdot x_1 + a_2^k \\cdot x_2 + \\dots + a_n^k \\cdot x_n + b^k\n",
    "    \\end{cases} \\, ,\n",
    "$$\n",
    "\n",
    "and in matrix form\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} \n",
    "y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_k\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "a_1^1 & a_2^1 & \\cdots & a_n^1 \\\\  \n",
    "a_1^2 & a_2^2 & \\cdots & a_n^2 \\\\  \n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\  \n",
    "a_1^k & a_2^k & \\cdots & a_n^k \\\\  \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n\n",
    "\\end{bmatrix} +\n",
    "\\begin{bmatrix} \n",
    "b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_k \n",
    "\\end{bmatrix} \\, ,\n",
    "$$\n",
    "\n",
    "Even shorter with matrix form:\n",
    "\n",
    "$$y = Ax + b \\, ,$$\n",
    "\n",
    "where $y \\in \\mathbb{R}^k$, $x \\in \\mathbb{R}^n$, and $b \\in \\mathbb{R}^k$ are vectors. $A \\in \\mathbb{R}^{k \\times n}$ is a matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d76e0a-40b9-4ee5-a88c-7eaaeb41905e",
   "metadata": {},
   "source": [
    "### It's time to introduce multilayer perceptron!\n",
    "\n",
    "Two layer MLP:\n",
    "\n",
    "<img src=\"media/mlp2.png\" width=\"300\" height=\"200\">\n",
    "\n",
    "#### Lets assign some sense to dots and lines.\n",
    "\n",
    "(i) Number layers\n",
    "\n",
    "<img src=\"media/mlp_layered.png\" width=\"300\" height=\"200\">\n",
    "\n",
    "(ii) Single output linear operation\n",
    "\n",
    "<p float=\"left\">\n",
    "    <img src=\"media/layer_1_1.png\" width=\"300\" height=\"200\" />\n",
    "    <img src=\"media/layer_1_2.png\" width=\"300\" height=\"200\" />\n",
    "</p>\n",
    "\n",
    "(iii) Matrix form of layer 1\n",
    "\n",
    "<img src=\"media/mlp_layer1.png\" width=\"450\" height=\"300\" />\n",
    "\n",
    "(iv) Matrix form of layer 2\n",
    "\n",
    "<img src=\"media/mlp_layer2.png\" width=\"450\" height=\"300\" />\n",
    "\n",
    "Such pictures always exclude a bias term $b$. That is why we do not have it in our equations, but we will use it further."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a324d63-ea79-468f-b056-7c04e2647f48",
   "metadata": {},
   "source": [
    "#### Is that it?\n",
    "\n",
    "<img src=\"media/mlp2.png\" width=\"300\" height=\"200\">\n",
    "\n",
    "This is 2-layered MLP looks like that?\n",
    "\n",
    "$$\\hat{y} = A(Ax + b) + b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5b4231-f456-4ab0-b997-f8d40003ff9b",
   "metadata": {},
   "source": [
    "**It is not.** Since linear combination of linear combination is still linear:\n",
    "\n",
    "$$\n",
    "\\hat{y} = A(Ax + b) + b = AAx + Ab + b,\n",
    "$$\n",
    "\n",
    "name $AA = A'\\, , \\, Ab + b = b'$.\n",
    "\n",
    "$$\n",
    "\\hat{y} = A(Ax + b) + b = A'x + b'.\n",
    "$$\n",
    "\n",
    "To obtain final MLP we need a **nonlinearity $\\sigma$**. Some nonlinear function that is used in between of linear layers to make our neural network capable of learning onlinear relationshions.\n",
    "\n",
    "#### Final formula of 2-layered perceptron\n",
    "\n",
    "$$\n",
    "\\hat{y} = A\\big(\\sigma(Ax + b)\\big) + b.\n",
    "$$\n",
    "\n",
    "So MLP is a sequence of linear functions with nonlinearity if between of them.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ab54bd-c465-4d58-a3e8-143c16c278d2",
   "metadata": {},
   "source": [
    "## Nonlinearities\n",
    "\n",
    "Used to be popular: `tanh()`, `sigmoid()`:\n",
    "\n",
    "$$\\texttt{sigmoid}(x) = \\frac{1}{1 + e^{-x}}.$$\n",
    "\n",
    "Now alternatives of **ReLU** (rectified linear unit) are used:\n",
    "\n",
    "$$\\texttt{ReLU}(x) = \\max(0, x).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb67a85b-9d45-4a9b-a537-871d09235655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return x * (x > 0)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(x) ** (-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af2e7a9-3400-45cf-888e-03b7b8eb5165",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-10, 10, 100)\n",
    "\n",
    "_, axes = plt.subplots(1, 2, figsize=(11, 4))\n",
    "\n",
    "axes[0].plot(x, sigmoid(x))\n",
    "axes[0].set_title('Sigmoid', fontsize=15)\n",
    "\n",
    "axes[1].plot(x, relu(x))\n",
    "axes[1].set_title('ReLU', fontsize=15)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908c98f3-77d9-4ebd-81dd-2ebbca9e4dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_torch = torch.linspace(-3, 3, 50)\n",
    "_, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].plot(x_torch, nn.ReLU()(x_torch))\n",
    "axes[0].plot(x_torch, [0]*50, '--', c='grey')\n",
    "axes[0].set_title('Torch ReLU', fontsize=15)\n",
    "\n",
    "axes[1].plot(x_torch, nn.LeakyReLU(0.05)(x_torch))\n",
    "axes[1].plot(x_torch, [0]*50, '--', c='grey')\n",
    "axes[1].set_title('Torch Leaky ReLU', fontsize=15)\n",
    "\n",
    "axes[2].plot(x_torch, nn.GELU()(x_torch))\n",
    "axes[2].plot(x_torch, [0]*50, '--', c='grey')\n",
    "axes[2].set_title('Torch GeLU', fontsize=15)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d2b1be-9e39-4dfe-a5f1-d43f8281fdc1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f902021f-82e5-4561-b962-ddb2a849310b",
   "metadata": {},
   "source": [
    "## Okay, it looks like that. How does it know things?\n",
    "\n",
    "\n",
    "### Loss functions\n",
    "\n",
    "Now we have an \"arbitrary\" function, and we have to solve a minimization problem.\n",
    "\n",
    "Again, we solve superivsed learning task: $y$ - observed values, $\\hat{y}$ - approximation with NN.\n",
    "\n",
    "Our task is find a minimum of a **loss function** (also called cost or error function). There is a MSE loss:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(x) = \\frac{1}{N} \\sum_1^N \\big( y - f(x) \\big)^2\n",
    "$$\n",
    "\n",
    "And many others for differents tasks. Check [them out](https://pytorch.org/docs/stable/nn.html#loss-functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e9f7a4-6072-4a58-84b4-d702705bab42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_mse(y_true, y_pred):\n",
    "    dif = y_true - y_pred\n",
    "    return torch.sum(dif * dif) / len(dif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d12e9d1-82a7-430f-8940-3557d785b4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = torch.linspace(10, 50, 200)\n",
    "y_pred = y_true + torch.rand(200) * 25\n",
    "\n",
    "print(f'My MSE:{my_mse(y_true, y_pred): .5f}')\n",
    "print(f'Torch MSE:{nn.MSELoss()(y_true, y_pred): .5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9082fb12-026d-42d3-b8ff-f98160a1322d",
   "metadata": {},
   "source": [
    "We want our error to be the smallest possible, so we aim to solve the problem:\n",
    "\n",
    "$$\n",
    "\\min_x (\\mathcal{L}(x))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a80f3a-deb7-4df4-babc-3864ff6f9ce7",
   "metadata": {},
   "source": [
    "### Gradient descent\n",
    "\n",
    "**Gradient** shows the direction of a function growth. *Pictures are from [wiki](https://en.wikipedia.org/wiki/Gradient).*\n",
    "\n",
    "<img src=\"media/grad1.png\" width=\"400\" height=\"300\" />\n",
    "<img src=\"media/grad2.png\" width=\"550\" height=\"300\" />\n",
    "<img src=\"media/grad3.png\" width=\"400\" height=\"300\" />\n",
    "\n",
    "\n",
    "Hence, if we will follow the gradient, we will find the maximum. Let's go in *an opposite direction* to the gradient:\n",
    "\n",
    "$$\n",
    "x_{i+1} = x_i - \\epsilon \\nabla f(x),\n",
    "$$\n",
    "\n",
    "where $\\nabla f(x)$ is a gradient of a function $f(x)$. $\\epsilon$ is a so-called *learning rate*. It controls updates: small updates will converge slowly, big updates may mess things up.\n",
    "\n",
    "<img src=\"media/lr.png\" width=\"400\" height=\"300\" />\n",
    "\n",
    "Now there is plenty of gradient descent methods variations. Check [this out](https://pytorch.org/docs/stable/optim.html#algorithms).\n",
    "\n",
    "### What we actually do with gradient descent is a weight updating\n",
    "\n",
    "We can say that our NN is parametized by some weights $\\theta$. Then, we would like to get such weights $\\theta$ that will produce the smallest error (i.e. loss):\n",
    "\n",
    "$$\n",
    "\\theta = \\text{argmin}_{\\theta} (\\mathcal{L}(x, \\theta)) \n",
    "$$\n",
    "\n",
    "And with gradient descent we iteratively update the weights:\n",
    "\n",
    "$$\n",
    "\\theta_{i+1} = \\theta_i - \\epsilon \\nabla \\mathcal{L}(x),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dced346-2948-4256-a34b-b06c03158a86",
   "metadata": {},
   "source": [
    "<img src=\"media/saddle_point.gif\" width=\"400\" height=\"300\" />\n",
    "\n",
    "\n",
    "#### With this approach we will obtain such weights that will make our loss (error) function as small as possible, won't we? Well, theoretically yes...\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316f9a36-7e20-4324-97ae-744ca6987c8e",
   "metadata": {},
   "source": [
    "## Universal Approximation Theorem\n",
    "\n",
    "*Based on `Cybenko, George. \"Approximation by superpositions of a sigmoidal function.\" Mathematics of control, signals and systems 2.4 (1989): 303-314.`*\n",
    "\n",
    "The Universal Approximation Theorem claims that at least one neural network exists that can approximate any continuous function with arbitrary precision. There is quite a few similar theoretical works. Let's check on [Runge's function]():\n",
    "\n",
    "$$\n",
    "f(x) = \\frac{1}{1 + 25 x^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac77845a-df46-4d0c-8066-f47bec160305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runge_func(x):\n",
    "    return 1 / (1 + 25 * x * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794b3971-c967-4552-b769-0bd8f4a02ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(-1, 1, 100)\n",
    "y = runge_func(x)\n",
    "\n",
    "plt.plot(x, y, label=\"Runge's function\")\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cb0891-f47a-45e1-bbcf-efeef3558d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a20843d-7661-4192-9a7b-721b3074938b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with this parameters\n",
    "batch_size = 32           # How many points are treated together by gradient descent?\n",
    "hidden_neurons = 100      # Number of parameters in the NN\n",
    "learning_rate = 1e-3      # Step size of gradient descent\n",
    "n_epochs = 100            # Training duration\n",
    "num_x_points = 100        # Dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b9c610-6280-4124-b81e-ff0a936c5d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RungesDataset(Dataset):\n",
    "    def __init__(self, n_points):\n",
    "        super(RungesDataset, self).__init__()\n",
    "        self.n_points = n_points\n",
    "        self.x = torch.linspace(-1, 1, n_points)\n",
    "        self.y = runge_func(self.x)\n",
    "\n",
    "    def __len__(self, ):\n",
    "        return self.x.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        X, y = self.x[idx], self.y[idx]\n",
    "        return X, y\n",
    "    \n",
    "dataset = RungesDataset(num_x_points)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbc8084-92cb-42c5-a413-b7a895f37a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different models, losses and optimization methods\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(1, hidden_neurons, bias=True),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(hidden_neurons, 1, bias=False)\n",
    ")\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-3)\n",
    "loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94334967-82e1-4833-9c5e-7727edcb39f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "train_loss = np.array([])\n",
    "test_loss = np.array([])\n",
    "\n",
    "for epoch in trange(n_epochs):\n",
    "    loss_epoch_train = np.array([])\n",
    "    loss_epoch_test = np.array([])\n",
    "    \n",
    "    model.train(True)\n",
    "    for batch in loader:\n",
    "        X, y = batch[0].unsqueeze(1).to(device), batch[1].unsqueeze(1).to(device)\n",
    "        y_pred = model(X)\n",
    "        l = loss(y_pred, y)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss_epoch_train = np.append(loss_epoch_train, l.item())\n",
    "        l.backward()\n",
    "        opt.step()\n",
    "\n",
    "    model.eval()\n",
    "    for batch in loader:\n",
    "        X, y = batch[0].unsqueeze(1).to(device), batch[1].unsqueeze(1).to(device)\n",
    "        y_pred = model(X)\n",
    "        l = loss(y_pred, y)\n",
    "        \n",
    "        loss_epoch_test = np.append(loss_epoch_test, l.item())\n",
    "        \n",
    "    train_loss = np.append(train_loss, loss_epoch_train.mean())\n",
    "    test_loss = np.append(test_loss, loss_epoch_test.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b513b65-8c06-40e6-906c-130664116c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(n_epochs), train_loss, label='Train')\n",
    "plt.plot(range(n_epochs), test_loss, label='Test')\n",
    "plt.legend()\n",
    "plt.title('Loss');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6dae45-97f1-4b1a-ab00-293c50c1b0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval().cpu()\n",
    "x_true = torch.linspace(-1, 1, 1000)\n",
    "\n",
    "plt.plot(x_true, runge_func(x_true), label=\"Runge's function true\")\n",
    "plt.plot(x_true, model(x_true.reshape(-1, 1)).detach().numpy(), label=\"Runge's function NN\")\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc41afad-56a1-4a24-b595-19adbd82b049",
   "metadata": {},
   "source": [
    "This example does not say that the theorem wrong. Training NN is really sensitive to **hyperparameters setup** (and such theorems work in specific conditions):\n",
    "\n",
    "(i) Not suitable learning. Tuning parameters is such a headache:\n",
    "    \n",
    "- Number of hidden layers\n",
    "- Dimensionality of hidden layers\n",
    "- Activation functions\n",
    "- Weights initialization\n",
    "- Normalization\n",
    "- Regularization\n",
    "- Loss function\n",
    "- Optimization method\n",
    "- Parameters of optimization\n",
    "- Batch size\n",
    "- Learning procedure\n",
    "\n",
    "(ii) Problem of finding global minimum in the presence of local minima:\n",
    "\n",
    "<img src=\"media/local_global_min.png\" width=\"400\" height=\"300\" />\n",
    "\n",
    "Typically you do not create your own model, but adapt **existing one to your problem**. In certain cases if you solve a similar problem you may even take the pre-trained model (i.e. you do not train NN by yourself from the very beginning). It is called **fine-tuning**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c22d1af-64e1-4646-b669-9a4eba95ee85",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data vs Task Parallelism\n",
    "\n",
    "Why GPUs are used?\n",
    "\n",
    "**Data parallelism**. You have a large amount of data elements. Each data element (susbet of elements) need to be processed to produce result.\n",
    "\n",
    "*Example*: summation of vectors.\n",
    "\n",
    "**Task parallelism**. You have collection of tasks that need to be completed.\n",
    "\n",
    "*Example*: google chrome and telegram processes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e531266-ea47-48ad-bb13-1cfcf8f73088",
   "metadata": {},
   "source": [
    "## Automatic Differentiation\n",
    "\n",
    "Recall gradient descent:\n",
    "\n",
    "$$\n",
    "\\theta_{i+1} = \\theta_i - \\epsilon \\nabla \\mathcal{L}(x).\n",
    "$$\n",
    "\n",
    "To make a gradient descent we need, well, compute gradients. We may use automatic differentiation packages to compute derivatives. Lets check PyTorch autograd with function:\n",
    "\n",
    "$$\n",
    "f(x) = e^{2x}sin(-x)x^2 \\, ,\n",
    "$$\n",
    "\n",
    "$$\n",
    "f'(x) = - xe^{2x}(x\\cos(x) + 2\\sin(x) + 2x\\sin(x)) \\, .\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ef6000-4aaf-4b6e-bc9c-0521a1051a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def some_function(x):\n",
    "    a = torch.exp(2 * x)\n",
    "    b = torch.sin(-x)\n",
    "    c = x * x\n",
    "    return a * b * c\n",
    "\n",
    "def deriv_some_function(x):\n",
    "    a = torch.cos(x) * x\n",
    "    b = 2 * torch.sin(x)\n",
    "    c = 2 * torch.sin(x) * x\n",
    "    tot = - torch.exp(2 * x) * x * (a + b + c)\n",
    "    return tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7aeee51-1e59-4904-a1a1-bde6382c9108",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(5., requires_grad=True)\n",
    "y = some_function(x)\n",
    "print(f'My derivative = {deriv_some_function(x):.3f}')\n",
    "\n",
    "y.backward()\n",
    "print(f'Torch derivative = {x.grad:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f062e01e-3aa0-4286-9792-4637c0b75455",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_linspace = torch.linspace(7, 12.3, 1000)\n",
    "plt.plot(x_linspace, some_function(x_linspace), 'k--', label='Function')\n",
    "plt.plot(x_linspace, deriv_some_function(x_linspace), label='Analytical deriv')\n",
    "\n",
    "x_linspace = torch.linspace(7, 12.3, 1000, requires_grad=True)\n",
    "y_linspace = some_function(x_linspace)\n",
    "y_sum = torch.sum(y_linspace)\n",
    "y_sum.backward()\n",
    "\n",
    "plt.plot(x_linspace.detach().numpy(), x_linspace.grad, label='Torch deriv')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec879a56-94cb-4b7d-8120-d4b49fac58e5",
   "metadata": {},
   "source": [
    "### What is going on during one epoch?\n",
    "\n",
    "An epoch means training the neural network with all the training data for one cycle. In an epoch, we use all of the data exactly once. Learning of a NN may be decomposed on two parts:\n",
    "\n",
    "(i) **Forward pass**. Compute values:\n",
    "\n",
    "<img src=\"media/forward.jpg\" width=\"800\" height=\"300\" />\n",
    "\n",
    "(ii) **Backward pass**. Compute weights updates:\n",
    "\n",
    "<img src=\"media/backward.jpg\" width=\"800\" height=\"300\" />\n",
    "\n",
    "One forward pass and one backward pass are count as *one pass*.\n",
    "\n",
    "Recall training in one epoch:\n",
    "\n",
    "    for batch in loader:\n",
    "        X, y = batch[0].unsqueeze(1).to(device), batch[1].unsqueeze(1).to(device)\n",
    "        y_pred = model(X)\n",
    "        l = loss(y_pred, y)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss_epoch_train = np.append(loss_epoch_train, l.item())\n",
    "        l.backward()\n",
    "        opt.step()\n",
    "\n",
    "In step `l.backward()`, torch automatically calculates loss derivatives for all weights of our function that `requires_grad`. In step `opt.step()`, torch updates the weights with derivatives according to the gradient descent methods used. This is called **backpropogation** and it is an essence of neural network training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83bd506-dd56-4d93-8ef4-1b8ad22597a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}